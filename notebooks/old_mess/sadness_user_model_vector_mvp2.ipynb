{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed76be6c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdee09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "\n",
    "# Best effort: move models off GPU (harmless if already deleted)\n",
    "for name in [\"model\"]:\n",
    "    if name in globals() and getattr(globals()[name], \"to\", None):\n",
    "        try:\n",
    "            globals()[name].to(\"cpu\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# Remove common large refs if they exist\n",
    "for name in [\n",
    "    \"model\", \"tokenizer\", \"inputs\", \"outputs\", \"logits\",\n",
    "    \"past_key_values\", \"activation_cache\", \"cache\", \"td\"\n",
    "]:\n",
    "    if name in globals():\n",
    "        obj = globals().pop(name)\n",
    "        # Try to close trace/hook objects if present\n",
    "        for m in (\"close\", \"reset_hooks\", \"remove_hooks\"):\n",
    "            try:\n",
    "                getattr(obj, m)()\n",
    "            except Exception:\n",
    "                pass\n",
    "        del obj\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        with torch.cuda.device(i):\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "print(\"Requested GPU memory cleanup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325cdb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "from transformers import AutoModelForCausalLM  # needed because it's in a type annotation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d13745d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import ModuleHook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b93708d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TalkTuner modules available from: /workspace/MATS-research/notebooks/TalkTuner-chatbot-llm-dashboard/src\n"
     ]
    }
   ],
   "source": [
    "# (Optional) Access to TalkTuner classes if needed later; not required for capture\n",
    "import sys\n",
    "TT_SRC = \"/workspace/MATS-research/notebooks/TalkTuner-chatbot-llm-dashboard/src\"\n",
    "if TT_SRC not in sys.path:\n",
    "    sys.path.append(TT_SRC)\n",
    "try:\n",
    "    from dataset import ModuleHook  # unused here, we use TransformerLens hooks instead\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a08fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Fetching 4 files: 100%|██████████| 4/4 [02:55<00:00, 43.97s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 105.29it/s]\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-8B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Set the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7427679f",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "40bd6813",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_messages(user_text: str, control_probe: bool) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Create a list of chat messages in OpenAI-style roles.\n",
    "    If control_probe is False, we inject an assistant prefix to steer the next turn.\n",
    "    \"\"\"\n",
    "    SYSTEM_PROMPT = (\n",
    "        \"You are a helpful, respectful and honest assistant. \"\n",
    "        \"Always answer as helpfully as possible, while being safe. \"\n",
    "        \"Please ensure that your responses are socially unbiased and positive in nature. \"\n",
    "        \"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. \"\n",
    "        \"If you don't know the answer to a question, please don't share false information.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": user_text},\n",
    "    ]\n",
    "\n",
    "    # When NOT a control probe, you want the activations to be conditioned on\n",
    "    # a specific assistant prefix (same behavior you had with the appended string).\n",
    "    if not control_probe:\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"In terms of emotions, I think this user experiences\"\n",
    "        })\n",
    "\n",
    "    return messages\n",
    "\n",
    "def serialize_prompt(tokenizer, messages: list[dict], control_probe: bool) -> str:\n",
    "    \"\"\"\n",
    "    Turn chat messages into the exact string Llama 3 expects internally,\n",
    "    using the model’s official chat template.\n",
    "    \"\"\"\n",
    "    # add_generation_prompt means: add an empty assistant header at the end\n",
    "    # so the model is \"ready to speak\".\n",
    "    # Use True only when you did NOT include an assistant message above (control_probe=True).\n",
    "    add_gen = control_probe  # True -> no assistant message provided; False -> we already provided one\n",
    "\n",
    "    prompt_str = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,             # we want a string back, not token ids (yet)\n",
    "        add_generation_prompt=add_gen\n",
    "    )\n",
    "    return prompt_str\n",
    "\n",
    "#Tokenization converts your human-readable serialized prompt into the numerical IDs (plus masks) that the model actually consumes\n",
    "def tokenize_prompt(tokenizer, prompt_str: str, max_length: int = 8192):\n",
    "    \"\"\"\n",
    "    Convert the serialized prompt string into input_ids and attention_mask tensors.\n",
    "    \"\"\"\n",
    "    if tokenizer.pad_token is None and tokenizer.eos_token is not None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # common for Llama models\n",
    "\n",
    "    enc = tokenizer(\n",
    "        prompt_str,\n",
    "        truncation=True,\n",
    "        max_length=max_length,     # Llama-3 supports ~8k context\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return enc  # dict with input_ids (1, T) and attention_mask (1, T)\n",
    "\n",
    "def extract_last_token_hidden_states(model, outputs) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    outputs: the dict returned by model(..., output_hidden_states=True)\n",
    "    Returns a tensor stacking the last-token vector from each transformer layer.\n",
    "\n",
    "    Shape details:\n",
    "    hidden_states is typically a tuple of length (num_layers + 1):\n",
    "        [0] = embeddings (pre-layer)\n",
    "        [1..num_layers] = outputs after each transformer layer\n",
    "    Each element has shape (batch=1, seq_len, hidden_dim)\n",
    "\n",
    "    We’ll take indices 1..num_layers (exclude embeddings), last token only.\n",
    "    Result shape will be (num_layers, hidden_dim).\n",
    "    \"\"\"\n",
    "    hs = outputs[\"hidden_states\"]\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "    layer_vecs = []\n",
    "\n",
    "    # indices 1..num_layers map to transformer layers 1..num_layers\n",
    "    for i in range(1, num_layers + 1):\n",
    "        # take last token (-1) and drop seq dim -> (1, hidden_dim)\n",
    "        v = hs[i][:, -1, :]                      # (1, H)\n",
    "        v = v.detach().cpu().to(torch.float)     # store off-GPU, fp32\n",
    "        layer_vecs.append(v)\n",
    "\n",
    "    # Stack along layer dimension -> (num_layers, hidden_dim)\n",
    "    return torch.cat(layer_vecs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91feb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionTextDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            directory: str,\n",
    "            tokenizer: AutoTokenizer,\n",
    "            model: AutoModelForCausalLM,\n",
    "            label_to_id: dict = None,\n",
    "            control_probe: bool = False,\n",
    "            ):\n",
    "        # This code creates a list of file paths for all .txt files in the specified directory.\n",
    "        # These files are expected to contain prompts for different emotion categories (e.g., happiness, sadness).\n",
    "        # Each .txt file corresponds to a label (emotion), and the file name (without extension) is used as the label.\n",
    "        # The resulting list (self.file_paths) is later used to load and process the prompts for each emotion in the _load_in_data method.\n",
    "        self.file_paths = [\n",
    "            os.path.join(directory, f)\n",
    "            for f in os.listdir(directory)\n",
    "            if os.path.isfile(os.path.join(directory, f)) and f.endswith('.txt')\n",
    "        ]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = []\n",
    "        self.texts = []\n",
    "        self.acts = []\n",
    "        self.label_to_id = label_to_id\n",
    "        self.model = model\n",
    "        self.control_probe = control_probe\n",
    "        self._load_in_data()\n",
    "\n",
    "\n",
    "    def _load_in_data(self):\n",
    "        for file_path in self.file_paths:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                # lines = f.readlines()\n",
    "                lines = [line.strip() for line in f.readlines() if line.strip()]\n",
    "\n",
    "            label_name = Path(file_path).stem\n",
    "            label = self.label_to_id[label_name]\n",
    "\n",
    "            for raw_text in lines:\n",
    "                # 1) messages\n",
    "                messages = build_messages(raw_text, control_probe=self.control_probe)\n",
    "                # 2) serialize -> string\n",
    "                prompt_str = serialize_prompt(self.tokenizer, messages, control_probe=self.control_probe)\n",
    "\n",
    "                # keep EXACT text the model will see, for debugging/repro\n",
    "                self.texts.append(prompt_str)\n",
    "                self.labels.append(label)\n",
    "\n",
    "                # 3) features from the serialized string\n",
    "                feats = self._get_feats(prompt_str)\n",
    "                self.acts.append(feats)\n",
    "                \n",
    "    def _get_feats(self, prompt_str: str) -> torch.Tensor:\n",
    "        # Step 1. Tokenize serialized prompt string\n",
    "        enc = tokenize_prompt(self.tokenizer, prompt_str, max_length=8192)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Step 2. Register hooks on modules of interest\n",
    "            features = OrderedDict()\n",
    "            for name, module in self.model.named_modules():\n",
    "                if name.endswith(\".mlp\") or name.endswith(\".embed_tokens\"):\n",
    "                    features[name] = ModuleHook(module)\n",
    "\n",
    "            # Step 3. Run model forward pass\n",
    "            outputs = self.model(\n",
    "                input_ids=enc[\"input_ids\"].to(self.model.device),\n",
    "                attention_mask=enc[\"attention_mask\"].to(self.model.device),\n",
    "                output_hidden_states=True,\n",
    "                return_dict=True,\n",
    "            )\n",
    "\n",
    "            # Step 4. Remove hooks (they stay alive until explicitly closed)\n",
    "            for hook in features.values():\n",
    "                hook.close()\n",
    "\n",
    "        # Step 5. Extract last-token hidden states from all layers\n",
    "        last_acts = extract_last_token_hidden_states(self.model, outputs)\n",
    "\n",
    "        # At this point:\n",
    "        #   last_acts  -> residual stream hidden states (per layer, last token)\n",
    "        #   features   -> contains hooked activations (per module, per forward call)\n",
    "\n",
    "        # Right now you only return hidden states\n",
    "        # but you can later decide to also return `features` if needed.\n",
    "        return last_acts\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        label = self.labels[idx]\n",
    "        text = self.texts[idx]\n",
    "        last_acts = self.acts[idx]\n",
    "\n",
    "        return {\n",
    "            'hidden_states': last_acts,\n",
    "            'label': label,\n",
    "            'text': text,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1881b9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.00it/s]\n",
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "# Llama-3 typically has eos but not pad; set pad to eos for batching\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,   # or bfloat16 if supported\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "39d36d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861c7ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_label = [\"happiness\", \"sadness\"]\n",
    "label_to_id = {\n",
    "               \"happiness\": 0,\n",
    "               \"sadness\": 1\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9319efb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

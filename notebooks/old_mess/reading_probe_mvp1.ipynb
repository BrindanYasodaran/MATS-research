{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7da22f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/MATS-research/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e1ab7",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05be432c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Environment Setup ---\n"
     ]
    }
   ],
   "source": [
    "# Suppress a specific UserWarning from SentencePiece\n",
    "warnings.filterwarnings(\"ignore\", message=r\".*Ignoring tokenizer_config\\.json since it is not set\\. It is likely that you are loading a tokenizer from a previous version of the library which does not contain this file\\..*\")\n",
    "\n",
    "print(\"--- Environment Setup ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f88df72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detected: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available and set the device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    # Clear cache to free up memory on the GPU\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"No GPU detected. Using CPU. This will be very slow.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de14fd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Model: meta-llama/Meta-Llama-3-8B-Instruct ---\n",
      "This will download and load ~16 GB of model weights. This may take several minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 38.59it/s]\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-8B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(f\"\\n--- Loading Model: {model_name} ---\")\n",
    "print(\"This will download and load ~16 GB of model weights. This may take several minutes.\")\n",
    "\n",
    "# Load the model directly using HookedTransformer.\n",
    "# We do NOT use quantization (`load_in_4bit`).\n",
    "# `torch_dtype=torch.bfloat16` is recommended for performance and is supported by the 3090.\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    model_name,\n",
    "    device=device,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # No quantization arguments needed!\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a11882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46c13ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verifying Model and Tokenizer ---\n",
      "Test prompt: Hello, world! This is a test.\n",
      "Tokenized input shape: torch.Size([1, 10])\n",
      "String tokens: ['<|begin_of_text|>', 'Hello', ',', ' world', '!', ' This', ' is', ' a', ' test', '.']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass successful!\n",
      "Logits shape: torch.Size([1, 10, 128256])\n",
      "Model's next token prediction for the prompt: ' I'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Verifying Model and Tokenizer ---\")\n",
    "test_prompt = \"Hello, world! This is a test.\"\n",
    "test_tokens = model.to_tokens(test_prompt)\n",
    "test_str_tokens = model.to_str_tokens(test_prompt)\n",
    "\n",
    "print(\"Test prompt:\", test_prompt)\n",
    "print(\"Tokenized input shape:\", test_tokens.shape)\n",
    "print(\"String tokens:\", test_str_tokens)\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        logits = model(test_tokens)\n",
    "    print(\"Forward pass successful!\")\n",
    "    print(\"Logits shape:\", logits.shape)\n",
    "\n",
    "    generated_token_id = logits[0, -1].argmax().item()\n",
    "    generated_token_str = tokenizer.decode(generated_token_id)\n",
    "    print(f\"Model's next token prediction for the prompt: '{generated_token_str}'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\nAn error occurred during the verification forward pass:\")\n",
    "    print(e)\n",
    "    print(\"If this is a CUDA out of memory error, your GPU does not have enough VRAM for the non-quantized model.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae827507",
   "metadata": {},
   "source": [
    "### Extracting  Layer Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aef57fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_conversation(text: str, user_identifier=\"HUMAN:\", ai_identifier=\"ASSISTANT:\") -> tuple[list[str], list[str]]:\n",
    "    \"\"\"\n",
    "    Splits a raw text conversation into a list of user messages and a list of assistant messages.\n",
    "    This logic is adapted from the paper's repository.\n",
    "    \"\"\"\n",
    "    user_messages, assistant_messages = [], []\n",
    "    lines = text.split(\"\\n\")\n",
    "    current_user_message, current_assistant_message = \"\", \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.lstrip(\" \")\n",
    "        if line.startswith(user_identifier):\n",
    "            if current_assistant_message:\n",
    "                assistant_messages.append(current_assistant_message.strip())\n",
    "            current_assistant_message = \"\"\n",
    "            current_user_message += line.replace(user_identifier, \"\").strip() + \" \"\n",
    "        elif line.startswith(ai_identifier):\n",
    "            if current_user_message:\n",
    "                user_messages.append(current_user_message.strip())\n",
    "            current_user_message = \"\"\n",
    "            current_assistant_message += line.replace(ai_identifier, \"\").strip() + \" \"\n",
    "\n",
    "    if current_user_message: user_messages.append(current_user_message.strip())\n",
    "    if current_assistant_message: assistant_messages.append(current_assistant_message.strip())\n",
    "        \n",
    "    return user_messages, assistant_messages\n",
    "\n",
    "def llama_v2_prompt(messages: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Formats a conversation into the LLaMA v2 prompt format.\n",
    "    This logic is adapted from the paper's repository.\n",
    "    \"\"\"\n",
    "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "    BOS, EOS = \"<s>\", \"</s>\"\n",
    "    DEFAULT_SYSTEM_PROMPT = \"You are a helpful, respectful and honest assistant.\"\n",
    "\n",
    "    if not messages or messages[0][\"role\"] != \"system\":\n",
    "        messages = [{\"role\": \"system\", \"content\": DEFAULT_SYSTEM_PROMPT}] + messages\n",
    "        \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": messages[1][\"role\"],\n",
    "            \"content\": B_SYS + messages[0][\"content\"] + E_SYS + messages[1][\"content\"],\n",
    "        }\n",
    "    ] + messages[2:]\n",
    "\n",
    "    messages_list = [\n",
    "        f\"{BOS}{B_INST} {(prompt['content']).strip()} {E_INST} {(answer['content']).strip()} {EOS}\"\n",
    "        for prompt, answer in zip(messages[::2], messages[1::2])\n",
    "    ]\n",
    "    \n",
    "    if messages and messages[-1][\"role\"] == \"user\":\n",
    "        messages_list.append(f\"{BOS}{B_INST} {(messages[-1]['content']).strip()} {E_INST}\")\n",
    "\n",
    "    return \"\".join(messages_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b713d233",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The paper's special prompt for coaxing out the gender representation\n",
    "prompt_translator = {\"_gender_\": \"gender\"}\n",
    "attribute = \"_gender_\"\n",
    "probe_prompt = f\" I think the {prompt_translator[attribute]} of this user is\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df4e155b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /workspace/MATS-research/data/chen_llama_gender\n",
      "Extracting activations from layers: [30]\n",
      "\n",
      "Found 1000 total conversation files.\n",
      "Running a test on the first 10 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Full Run: Extracting Activations: 100%|██████████| 1000/1000 [01:35<00:00, 10.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully processed 500 conversations.\n",
      "\n",
      "--- Verifying Extracted Data ---\n",
      "Shape of a single activation tensor: torch.Size([4096])\n",
      "Final label distribution: Counter({'female': 250, 'male': 250})\n",
      "Number of activations matches number of labels.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3.2: Configuration and Test Run ---\n",
    "\n",
    "# ====================================================================\n",
    "# Configuration\n",
    "# ====================================================================\n",
    "# Specify which layers you want to extract activations from.\n",
    "# Llama-3-8B has 32 layers (0-31).\n",
    "LAYERS_TO_EXTRACT = [30] # Example: 2nd to last layer\n",
    "\n",
    "# Define the path to your dataset folder\n",
    "dataset_path = \"/workspace/MATS-research/data/chen_llama_gender\"\n",
    "\n",
    "# Number of files to run for this test.\n",
    "NUM_TEST_FILES = 10 \n",
    "# ====================================================================\n",
    "\n",
    "print(f\"Loading data from: {dataset_path}\")\n",
    "print(f\"Extracting activations from layers: {LAYERS_TO_EXTRACT}\\n\")\n",
    "\n",
    "conversation_files = [f for f in os.listdir(dataset_path) if f.endswith('.txt')]\n",
    "print(f\"Found {len(conversation_files)} total conversation files.\")\n",
    "print(f\"Running a test on the first {NUM_TEST_FILES} files...\")\n",
    "\n",
    "# Use a slice of the files for the test run\n",
    "# test_files_subset = conversation_files[:NUM_TEST_FILES]\n",
    "\n",
    "# Lists to store our test data\n",
    "test_activations = []\n",
    "test_labels = []\n",
    "\n",
    "# This filter function tells transformer_lens to only cache the activations we need\n",
    "def names_filter(name: str):\n",
    "    is_resid_post = name.endswith(\"resid_post\")\n",
    "    if not is_resid_post: return False\n",
    "    \n",
    "    layer_index = int(name.split('.')[1])\n",
    "    return layer_index in LAYERS_TO_EXTRACT\n",
    "\n",
    "\n",
    "# --- Step 3.3: Full Data Extraction ---\n",
    "\n",
    "# Lists to store our final data\n",
    "all_activations = []\n",
    "all_labels = []\n",
    "\n",
    "# This loop uses the full 'conversation_files' list\n",
    "for file_name in tqdm(conversation_files, desc=\"Full Run: Extracting Activations\"):\n",
    "    file_path = os.path.join(dataset_path, file_name)\n",
    "    \n",
    "    if \"_gender_female\" in file_name:\n",
    "        label = \"female\"\n",
    "    elif \"_gender_male\" in file_name:\n",
    "        label = \"male\"\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        raw_text = f.read()\n",
    "\n",
    "    # Re-using the same helper functions as before\n",
    "    user_msgs, ai_msgs = split_conversation(raw_text)\n",
    "    messages_dict = []\n",
    "    for user_msg, ai_msg in zip(user_msgs, ai_msgs):\n",
    "        messages_dict.append({'role': 'user', 'content': user_msg})\n",
    "        messages_dict.append({'role': 'assistant', 'content': ai_msg})\n",
    "        \n",
    "    if not messages_dict:\n",
    "        continue\n",
    "\n",
    "    full_prompt = llama_v2_prompt(messages_dict) + probe_prompt\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(full_prompt, names_filter=names_filter)\n",
    "        \n",
    "        activations_for_prompt = torch.stack(\n",
    "            [cache[f\"blocks.{layer}.hook_resid_post\"][0, -1, :] for layer in LAYERS_TO_EXTRACT],\n",
    "            dim=0\n",
    "        )\n",
    "        if len(LAYERS_TO_EXTRACT) == 1:\n",
    "            activations_for_prompt = activations_for_prompt.squeeze(0)\n",
    "\n",
    "        all_activations.append(activations_for_prompt.cpu())\n",
    "        all_labels.append(label)\n",
    "\n",
    "print(f\"\\nSuccessfully processed {len(all_activations)} conversations.\")\n",
    "\n",
    "# --- Sanity Checks for the Full Run ---\n",
    "print(\"\\n--- Verifying Extracted Data ---\")\n",
    "if all_activations:\n",
    "    print(f\"Shape of a single activation tensor: {all_activations[0].shape}\")\n",
    "    label_counts = Counter(all_labels)\n",
    "    print(f\"Final label distribution: {label_counts}\")\n",
    "    assert len(all_activations) == len(all_labels)\n",
    "    print(\"Number of activations matches number of labels.\")\n",
    "else:\n",
    "    print(\"No data was processed in the full run.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "746bf800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 files that were skipped (as expected).\n",
      "\n",
      "Here are the first 10 skipped files:\n",
      "conversation_250_age_female.txt\n",
      "conversation_250_age_male.txt\n",
      "conversation_251_age_female.txt\n",
      "conversation_251_age_male.txt\n",
      "conversation_252_age_female.txt\n",
      "conversation_252_age_male.txt\n",
      "conversation_253_age_female.txt\n",
      "conversation_253_age_male.txt\n",
      "conversation_254_age_female.txt\n",
      "conversation_254_age_male.txt\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/workspace/MATS-research/data/chen_llama_gender\"\n",
    "conversation_files = [f for f in os.listdir(dataset_path) if f.endswith('.txt')]\n",
    "\n",
    "skipped_files = []\n",
    "for file_name in conversation_files:\n",
    "    if \"_gender_female\" not in file_name and \"_gender_male\" not in file_name:\n",
    "        skipped_files.append(file_name)\n",
    "\n",
    "print(f\"Found {len(skipped_files)} files that were skipped (as expected).\")\n",
    "print(\"\\nHere are the first 10 skipped files:\")\n",
    "for file in skipped_files[:10]:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc51e5b6",
   "metadata": {},
   "source": [
    "### Preparing the Data for Probe Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70f052af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples to process: 500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_samples = len(all_activations)\n",
    "print(f\"Number of samples to process: {num_samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8517a6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stacked activations tensor shape: torch.Size([500, 4096])\n",
      "Labels tensor shape: torch.Size([500])\n",
      "First 10 numerical labels: [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]\n",
      "\n",
      "--- Data Splitting Complete ---\n",
      "Training data shape (X_train): torch.Size([400, 4096])\n",
      "Training labels shape (y_train): torch.Size([400])\n",
      "Testing data shape (X_test):  torch.Size([100, 4096])\n",
      "Testing labels shape (y_test):  torch.Size([100])\n",
      "\n",
      "Training set label balance: 0.50 (proportion of 'male')\n",
      "Test set label balance:     0.50 (proportion of 'male')\n",
      "\n",
      "--- Step 4 Complete ---\n",
      "You now have your data prepared in X_train, X_test, y_train, and y_test.\n",
      "You are ready to proceed to Step 5: Defining and Training the Linear Probes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Stack the list of activation tensors into a single large tensor.\n",
    "# Each tensor in the list is for one conversation. We stack them along a new 'batch' dimension.\n",
    "# The original list contained tensors of shape [d_model], so the final shape will be [num_samples, d_model].\n",
    "activations_tensor = torch.stack(all_activations)\n",
    "print(f\"\\nStacked activations tensor shape: {activations_tensor.shape}\")\n",
    "\n",
    "# 2. Convert the string labels ('female', 'male') into numerical format (0, 1).\n",
    "# This is required for training a classifier. We'll use a simple mapping.\n",
    "label_map = {\"female\": 0, \"male\": 1}\n",
    "labels_numerical = [label_map[label] for label in all_labels]\n",
    "labels_tensor = torch.tensor(labels_numerical, dtype=torch.float32) # Use float32 for BCEWithLogitsLoss later\n",
    "print(f\"Labels tensor shape: {labels_tensor.shape}\")\n",
    "print(f\"First 10 numerical labels: {labels_tensor[:10].int().tolist()}\")\n",
    "\n",
    "# 3. Split the data into training and testing sets.\n",
    "# An 80/20 split is a standard choice.\n",
    "# - 'random_state=42' ensures that the split is the same every time we run the code, which is crucial for reproducibility.\n",
    "# - 'stratify=labels_tensor' ensures that the proportion of male/female labels is the same in both the train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    activations_tensor, \n",
    "    labels_tensor, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=labels_tensor\n",
    ")\n",
    "\n",
    "print(\"\\n--- Data Splitting Complete ---\")\n",
    "print(f\"Training data shape (X_train): {X_train.shape}\")\n",
    "print(f\"Training labels shape (y_train): {y_train.shape}\")\n",
    "print(f\"Testing data shape (X_test):  {X_test.shape}\")\n",
    "print(f\"Testing labels shape (y_test):  {y_test.shape}\")\n",
    "\n",
    "# Verify stratification by checking the balance of labels in each set\n",
    "print(f\"\\nTraining set label balance: {y_train.sum()/len(y_train):.2f} (proportion of 'male')\")\n",
    "print(f\"Test set label balance:     {y_test.sum()/len(y_test):.2f} (proportion of 'male')\")\n",
    "\n",
    "print(\"\\n--- Step 4 Complete ---\")\n",
    "print(\"You now have your data prepared in X_train, X_test, y_train, and y_test.\")\n",
    "print(\"You are ready to proceed to Step 5: Defining and Training the Linear Probes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4639f8",
   "metadata": {},
   "source": [
    "### Defining and Training the Linear Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8f09a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearProbe class defined.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the Linear Probe model as a PyTorch Module\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        # A single linear layer that maps from the activation dimension to a single logit\n",
    "        self.probe = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # We squeeze the output to remove the last dimension, so it has shape [batch_size]\n",
    "        # which is what our loss function expects.\n",
    "        return self.probe(x).squeeze(-1)\n",
    "\n",
    "print(\"LinearProbe class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b58a6bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_probe function defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5.2: Define the Training Function ---\n",
    "\n",
    "def train_probe(probe, X_train, y_train, epochs=40, lr=1e-3, batch_size=32):\n",
    "    \"\"\"\n",
    "    Trains a single linear probe.\n",
    "    \n",
    "    Args:\n",
    "        probe (LinearProbe): The probe to train.\n",
    "        X_train (Tensor): The training activations.\n",
    "        y_train (Tensor): The training labels.\n",
    "        epochs (int): Number of training epochs.\n",
    "        lr (float): Learning rate.\n",
    "        batch_size (int): Batch size for training.\n",
    "        \n",
    "    Returns:\n",
    "        LinearProbe: The trained probe.\n",
    "    \"\"\"\n",
    "    # Move probe to the correct device\n",
    "    probe.to(device)\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    # BCEWithLogitsLoss is perfect for binary classification from a single logit.\n",
    "    # It's numerically stable and combines a Sigmoid layer and Binary Cross Entropy loss.\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.AdamW(probe.parameters(), lr=lr)\n",
    "    \n",
    "    # Create a DataLoader for batching\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            # Move data to the GPU\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            X_batch = X_batch.to(torch.float32) \n",
    "            \n",
    "            # Standard PyTorch training loop\n",
    "            optimizer.zero_grad()\n",
    "            logits = probe(X_batch)\n",
    "            loss = loss_fn(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "            \n",
    "    return probe\n",
    "\n",
    "print(\"train_probe function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "999fb9af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training a single probe for Layer 30 ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Loss: 0.4146\n",
      "Epoch 20/50, Loss: 0.3878\n",
      "Epoch 30/50, Loss: 0.3444\n",
      "Epoch 40/50, Loss: 0.3436\n",
      "Epoch 50/50, Loss: 0.3373\n",
      "\n",
      "--- Training Complete ---\n",
      "Trained 1 probes for layers: [30]\n",
      "\n",
      "--- Step 5 Complete ---\n",
      "You are now ready to proceed to Step 6: Evaluating the Probes and Visualizing the Results.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5.3: Main Training Loop ---\n",
    "\n",
    "# This dictionary will store our trained probes, mapping the layer index to the probe model\n",
    "trained_probes = {}\n",
    "input_dim = model.cfg.d_model # Should be 4096 for Llama-3-8B\n",
    "\n",
    "# This code handles both cases: a single layer's activations or multiple layers'\n",
    "if X_train.ndim == 2:\n",
    "    # Case 1: You extracted from a single layer. X_train has shape [num_samples, d_model].\n",
    "    layer_index = LAYERS_TO_EXTRACT[0]\n",
    "    print(f\"--- Training a single probe for Layer {layer_index} ---\")\n",
    "    \n",
    "    probe = LinearProbe(input_dim)\n",
    "    trained_probe = train_probe(probe, X_train, y_train, epochs=50)\n",
    "    trained_probes[layer_index] = trained_probe\n",
    "    \n",
    "elif X_train.ndim == 3:\n",
    "    # Case 2: You extracted from multiple layers. X_train has shape [num_samples, num_layers, d_model].\n",
    "    num_layers_extracted = X_train.shape[1]\n",
    "    print(f\"--- Training one probe for each of the {num_layers_extracted} extracted layers ---\")\n",
    "    \n",
    "    for i, layer_index in enumerate(LAYERS_TO_EXTRACT):\n",
    "        print(f\"\\n--- Training Probe for Layer {layer_index} ---\")\n",
    "        \n",
    "        # Get the activations for this specific layer\n",
    "        layer_activations = X_train[:, i, :]\n",
    "        \n",
    "        probe = LinearProbe(input_dim)\n",
    "        trained_probe = train_probe(probe, layer_activations, y_train)\n",
    "        trained_probes[layer_index] = trained_probe\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")\n",
    "print(f\"Trained {len(trained_probes)} probes for layers: {list(trained_probes.keys())}\")\n",
    "\n",
    "print(\"\\n--- Step 5 Complete ---\")\n",
    "print(\"You are now ready to proceed to Step 6: Evaluating the Probes and Visualizing the Results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cba731d",
   "metadata": {},
   "source": [
    "### Evaluating the Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05d8c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_probe(probe, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluates a single trained linear probe on the test set.\n",
    "    \n",
    "    Args:\n",
    "        probe (LinearProbe): The trained probe to evaluate.\n",
    "        X_test (Tensor): The testing activations.\n",
    "        y_test (Tensor): The testing labels.\n",
    "        \n",
    "    Returns:\n",
    "        float: The accuracy of the probe on the test set.\n",
    "    \"\"\"\n",
    "    # Set the probe to evaluation mode\n",
    "    probe.eval()\n",
    "    probe.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Move test data to the GPU and ensure correct dtype\n",
    "        X_test_gpu = X_test.to(device).to(torch.float32)\n",
    "        y_test_gpu = y_test.to(device)\n",
    "        \n",
    "        # Get logits from the probe\n",
    "        logits = probe(X_test_gpu)\n",
    "        \n",
    "        # Convert logits to predictions (0 or 1)\n",
    "        # A positive logit corresponds to a prediction of 1 ('male')\n",
    "        predictions = (logits > 0).int()\n",
    "        \n",
    "        # Calculate accuracy by comparing predictions to true labels\n",
    "        accuracy = (predictions == y_test_gpu.int()).float().mean().item()\n",
    "        \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c6d8ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating Probe for Layer 30 ---\n",
      "\n",
      "Probe for Layer 30 - Test Accuracy: 77.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check that we have exactly one probe trained, as expected\n",
    "assert len(trained_probes) == 1, f\"Expected 1 trained probe, but found {len(trained_probes)}. Please re-run from Step 4.\"\n",
    "\n",
    "# Get the layer index and the trained probe model from our dictionary\n",
    "layer_index = list(trained_probes.keys())[0]\n",
    "probe_to_evaluate = trained_probes[layer_index]\n",
    "\n",
    "print(f\"--- Evaluating Probe for Layer {layer_index} ---\")\n",
    "\n",
    "# Calculate the test accuracy\n",
    "test_accuracy = evaluate_probe(probe_to_evaluate, X_test, y_test)\n",
    "\n",
    "print(f\"\\nProbe for Layer {layer_index} - Test Accuracy: {test_accuracy*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa7471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

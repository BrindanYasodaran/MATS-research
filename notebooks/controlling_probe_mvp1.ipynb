{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b641dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from functools import partial\n",
    "import warnings\n",
    "\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a8bc236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detected: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "# Check if a GPU is available and set the device\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"GPU detected: {torch.cuda.get_device_name(0)}\")\n",
    "    # Clear cache to free up memory on the GPU\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f8109a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading Model: meta-llama/Meta-Llama-3-8B-Instruct ---\n",
      "This will download and load ~16 GB of model weights. This may take several minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 36.62it/s]\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-8B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "print(f\"\\n--- Loading Model: {model_name} ---\")\n",
    "print(\"This will download and load ~16 GB of model weights. This may take several minutes.\")\n",
    "\n",
    "# Load the model directly using HookedTransformer.\n",
    "# `torch_dtype=torch.bfloat16` is recommended for performance and is supported by the 3090.\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    model_name,\n",
    "    device=device,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5a957",
   "metadata": {},
   "source": [
    "### Extract Activations for Control Probe Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6a60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_conversation(text: str, user_identifier=\"HUMAN:\", ai_identifier=\"ASSISTANT:\") -> tuple[list[str], list[str]]:\n",
    "    user_messages, assistant_messages = [], []\n",
    "    lines = text.split(\"\\n\")\n",
    "    current_user_message, current_assistant_message = \"\", \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.lstrip(\" \")\n",
    "        if line.startswith(user_identifier):\n",
    "            if current_assistant_message:\n",
    "                assistant_messages.append(current_assistant_message.strip())\n",
    "            current_assistant_message = \"\"\n",
    "            current_user_message += line.replace(user_identifier, \"\").strip() + \" \"\n",
    "        elif line.startswith(ai_identifier):\n",
    "            if current_user_message:\n",
    "                user_messages.append(current_user_message.strip())\n",
    "            current_user_message = \"\"\n",
    "            current_assistant_message += line.replace(ai_identifier, \"\").strip() + \" \"\n",
    "\n",
    "    if current_user_message: user_messages.append(current_user_message.strip())\n",
    "    if current_assistant_message: assistant_messages.append(current_assistant_message.strip())\n",
    "        \n",
    "    return user_messages, assistant_messages\n",
    "\n",
    "def llama_v3_prompt(messages: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Creates a prompt string formatted for Llama 3 Instruct models.\n",
    "    \"\"\"\n",
    "    prompt_parts = [\"<|begin_of_text|>\"]\n",
    "    \n",
    "    for message in messages:\n",
    "        role = message[\"role\"]\n",
    "        content = message[\"content\"]\n",
    "        prompt_parts.append(f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{content}<|eot_id|>\")\n",
    "        \n",
    "    # The prompt should end with the start of the assistant's turn\n",
    "    prompt_parts.append(\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\")\n",
    "    \n",
    "    return \"\".join(prompt_parts)\n",
    "\n",
    "#can also the following code in place of llama_v3_prompt\n",
    "# truncated_prompt = model.tokenizer.apply_chat_template(\n",
    "#     messages_dict,\n",
    "#     tokenize=False,\n",
    "#     add_generation_prompt=True\n",
    "# )\n",
    "\n",
    "\n",
    "# --- Configuration for this Step ---\n",
    "\n",
    "# Define the path to your dataset folder\n",
    "dataset_path = \"/workspace/MATS-research/data/chen_llama_gender\"\n",
    "\n",
    "# Define which layer to extract activations from. We'll use the same layer as before.\n",
    "LAYER_TO_EXTRACT = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ef61ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /workspace/MATS-research/data/chen_llama_gender\n",
      "Extracting activations from layer: 30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Activations: 100%|██████████| 1000/1000 [01:11<00:00, 13.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully processed 500 conversations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading data from: {dataset_path}\")\n",
    "print(f\"Extracting activations from layer: {LAYER_TO_EXTRACT}\\n\")\n",
    "\n",
    "conversation_files = [f for f in os.listdir(dataset_path) if f.endswith('.txt')]\n",
    "\n",
    "all_activations = []\n",
    "all_labels = []\n",
    "\n",
    "# Filter function to only cache the layer we need\n",
    "def names_filter(name: str):\n",
    "    return name == f\"blocks.{LAYER_TO_EXTRACT}.hook_resid_post\"\n",
    "\n",
    "for file_name in tqdm(conversation_files, desc=\"Extracting Activations\"):\n",
    "    file_path = os.path.join(dataset_path, file_name)\n",
    "    \n",
    "    if \"_gender_female\" in file_name:\n",
    "        label = \"female\"\n",
    "    elif \"_gender_male\" in file_name:\n",
    "        label = \"male\"\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        raw_text = f.read()\n",
    "\n",
    "    user_msgs, ai_msgs = split_conversation(raw_text)\n",
    "    messages_dict = []\n",
    "    for user_msg, ai_msg in zip(user_msgs, ai_msgs):\n",
    "        messages_dict.append({'role': 'user', 'content': user_msg})\n",
    "        messages_dict.append({'role': 'assistant', 'content': ai_msg})\n",
    "        \n",
    "    if not messages_dict:\n",
    "        continue\n",
    "    \n",
    "    # Truncate the conversation to end after the last user message\n",
    "    if messages_dict and messages_dict[-1]['role'] == 'assistant':\n",
    "        messages_dict = messages_dict[:-1]\n",
    "    \n",
    "    if not messages_dict: # If removing the last AI message leaves nothing, skip\n",
    "        continue\n",
    "\n",
    "    truncated_prompt = llama_v3_prompt(messages_dict)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(truncated_prompt, names_filter=names_filter)\n",
    "        \n",
    "        # We only need the activation from our specified layer at the final token position\n",
    "        activation = cache[f\"blocks.{LAYER_TO_EXTRACT}.hook_resid_post\"][0, -1, :]\n",
    "        \n",
    "        all_activations.append(activation.cpu())\n",
    "        all_labels.append(label)\n",
    "\n",
    "print(f\"\\nSuccessfully processed {len(all_activations)} conversations.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "efb8220d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing Data for Probe Training ---\n",
      "Training data shape (X_train_control): torch.Size([400, 4096])\n",
      "Test data shape (X_test_control):  torch.Size([100, 4096])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Prepare data for probe training ---\n",
    "print(\"\\n--- Preparing Data for Probe Training ---\")\n",
    "\n",
    "activations_tensor = torch.stack(all_activations)\n",
    "label_map = {\"female\": 0, \"male\": 1}\n",
    "labels_numerical = [label_map[label] for label in all_labels]\n",
    "labels_tensor = torch.tensor(labels_numerical, dtype=torch.float32)\n",
    "\n",
    "# We create new variable names to avoid confusion with the reading probe data\n",
    "X_train_control, X_test_control, y_train_control, y_test_control = train_test_split(\n",
    "    activations_tensor, \n",
    "    labels_tensor, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=labels_tensor\n",
    ")\n",
    "\n",
    "print(f\"Training data shape (X_train_control): {X_train_control.shape}\")\n",
    "print(f\"Test data shape (X_test_control):  {X_test_control.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab280662",
   "metadata": {},
   "source": [
    "### Train the Linear Probe and Extract the Control Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5550bccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions for training and evaluation are defined.\n"
     ]
    }
   ],
   "source": [
    "# --- Step B.1: Define Probe, Training, and Evaluation Functions ---\n",
    "\n",
    "# Define the Linear Probe model (same as before)\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.probe = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.probe(x).squeeze(-1)\n",
    "\n",
    "# Define the training function (with the dtype fix)\n",
    "def train_probe(probe, X_train, y_train, epochs=100, lr=1e-3, batch_size=32):\n",
    "    probe.to(device)\n",
    "    probe.train()\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.AdamW(probe.parameters(), lr=lr)\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            X_batch = X_batch.to(torch.float32)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = probe(X_batch)\n",
    "            loss = loss_fn(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(train_loader):.4f}\")\n",
    "            \n",
    "    return probe\n",
    "\n",
    "# Define the evaluation function (same as before)\n",
    "def evaluate_probe(probe, X_test, y_test):\n",
    "    probe.eval()\n",
    "    probe.to(device)\n",
    "    with torch.no_grad():\n",
    "        X_test_gpu = X_test.to(device).to(torch.float32)\n",
    "        y_test_gpu = y_test.to(device)\n",
    "        logits = probe(X_test_gpu)\n",
    "        predictions = (logits > 0).int()\n",
    "        accuracy = (predictions == y_test_gpu.int()).float().mean().item()\n",
    "    return accuracy\n",
    "\n",
    "print(\"Helper functions for training and evaluation are defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f685d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Control Probe for Layer 30 ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/40, Loss: 0.3106\n",
      "Epoch 40/40, Loss: 0.2405\n",
      "\n",
      "--- Verifying probe performance ---\n",
      "Control Probe Test Accuracy: 72.00%\n",
      "\n",
      "--- Control Vector Extracted ---\n",
      "Control vector shape: torch.Size([4096])\n",
      "Control vector norm: 2.60\n"
     ]
    }
   ],
   "source": [
    "# --- Step B.2: Train Probe and Extract Control Vector ---\n",
    "\n",
    "input_dim = model.cfg.d_model\n",
    "\n",
    "# Initialize a new probe specifically for this task\n",
    "control_probe_model = LinearProbe(input_dim)\n",
    "\n",
    "print(f\"--- Training Control Probe for Layer {LAYER_TO_EXTRACT} ---\")\n",
    "# Train the probe using the data from the end of user turns\n",
    "trained_control_probe = train_probe(\n",
    "    control_probe_model, \n",
    "    X_train_control, \n",
    "    y_train_control, \n",
    "    epochs=40\n",
    ")\n",
    "\n",
    "# --- Verification Step ---\n",
    "# It's crucial to verify the probe's accuracy on the test set.\n",
    "# If the probe can't classify gender, its weight vector is meaningless.\n",
    "print(\"\\n--- Verifying probe performance ---\")\n",
    "test_accuracy = evaluate_probe(trained_control_probe, X_test_control, y_test_control)\n",
    "print(f\"Control Probe Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "# --- Extraction Step ---\n",
    "# The control vector is the weight vector of the trained linear probe.\n",
    "# The probe has one linear layer called 'probe'. We access its 'weight' attribute.\n",
    "# .data gets the tensor without gradients.\n",
    "# .squeeze() removes the first dimension (shape [1, 4096] -> [4096]).\n",
    "# .detach() creates a new tensor that doesn't require gradients.\n",
    "control_vector = trained_control_probe.probe.weight.data.squeeze().detach()\n",
    "\n",
    "print(\"\\n--- Control Vector Extracted ---\")\n",
    "print(f\"Control vector shape: {control_vector.shape}\") # Should be [d_model], i.e., [4096]\n",
    "print(f\"Control vector norm: {control_vector.norm().item():.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8f801",
   "metadata": {},
   "source": [
    "### Setting Up the Intervention with Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "593da3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 30 questions from '/workspace/MATS-research/data/test_questions/gender.txt'.\n",
      "\n",
      "Sample questions:\n",
      "- Hi! How should I style my hair for a formal event? Suggest two hairstyles.\n",
      "- Hi llama! How can I select the right fragrance for me? Please give me at least three types of scents!\n",
      "- What should I wear on a first date? Please give me at least three specific outfit suggestions!\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load the Paper's Questions ---\n",
    "\n",
    "# Define the path to the text file containing the questions\n",
    "questions_file_path = \"/workspace/MATS-research/data/test_questions/gender.txt\"\n",
    "\n",
    "try:\n",
    "    with open(questions_file_path, 'r') as f:\n",
    "        # Read all lines, strip whitespace, and filter out any empty lines\n",
    "        gender_questions = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    print(f\"Successfully loaded {len(gender_questions)} questions from '{questions_file_path}'.\")\n",
    "    assert len(gender_questions) == 30, \"Expected 30 questions, but found a different number.\"\n",
    "    \n",
    "    print(\"\\nSample questions:\")\n",
    "    for q in gender_questions[:3]:\n",
    "        print(f\"- {q}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: The file '{questions_file_path}' was not found.\")\n",
    "    print(\"Please make sure you have created this file and it is in the same directory as your notebook.\")\n",
    "    # In case of error, create a dummy list to avoid breaking subsequent cells\n",
    "    gender_questions = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c4af60",
   "metadata": {},
   "source": [
    "### Running the Control Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "067af630",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def steering_hook(\n",
    "    resid_pre: torch.Tensor,\n",
    "    hook: HookPoint,\n",
    "    c_vec: torch.Tensor,\n",
    "    multiplier: float\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    A hook function to steer the model's activations.\n",
    "    It adds the control vector multiplied by a strength factor \n",
    "    to the residual stream at the final token position.\n",
    "    \"\"\"\n",
    "    # We only apply the steering vector to the activation of the LAST token.\n",
    "    resid_pre[:, -1, :] += c_vec * multiplier\n",
    "    return resid_pre\n",
    "\n",
    "def run_steering_experiment(\n",
    "    model: HookedTransformer,\n",
    "    questions: list[str],\n",
    "    control_vector: torch.Tensor,\n",
    "    layer: int,\n",
    "    multiplier: float,\n",
    "    max_new_tokens: int = 150\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Runs the steering experiment, cleanly separating the generated\n",
    "    completion from the initial prompt.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    hook_point = f\"blocks.{layer}.hook_resid_post\"\n",
    "\n",
    "    for question in tqdm(questions, desc=\"Generating Steered Responses\"):\n",
    "        messages = [{'role': 'user', 'content': question}]\n",
    "        \n",
    "        # Create the prompt string\n",
    "        prompt_str = model.tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize the prompt to get its length\n",
    "        prompt_tokens = model.to_tokens(prompt_str)\n",
    "        prompt_length = prompt_tokens.shape[1]\n",
    "\n",
    "        # --- 1. Baseline Generation (No Hooks) ---\n",
    "        # Generate tokens, not a string\n",
    "        baseline_output_tokens = model.generate(\n",
    "            prompt_tokens, \n",
    "            max_new_tokens=max_new_tokens, \n",
    "            verbose=False,\n",
    "            temperature=0.0,\n",
    "            return_type=\"tokens\" # Make sure to get tokens back\n",
    "        )\n",
    "        # Slice and decode ONLY the new tokens\n",
    "        baseline_completion = model.to_string(baseline_output_tokens[0, prompt_length:])\n",
    "\n",
    "        # --- 2. Male-Steered Generation ---\n",
    "        male_hook_fn = partial(steering_hook, c_vec=control_vector, multiplier=multiplier)\n",
    "        with model.hooks(fwd_hooks=[(hook_point, male_hook_fn)]):\n",
    "            male_steered_tokens = model.generate(\n",
    "                prompt_tokens, max_new_tokens=max_new_tokens, verbose=False, temperature=0.0, return_type=\"tokens\"\n",
    "            )\n",
    "        male_completion = model.to_string(male_steered_tokens[0, prompt_length:])\n",
    "\n",
    "        # --- 3. Female-Steered Generation ---\n",
    "        female_hook_fn = partial(steering_hook, c_vec=control_vector, multiplier=-multiplier)\n",
    "        with model.hooks(fwd_hooks=[(hook_point, female_hook_fn)]):\n",
    "            female_steered_tokens = model.generate(\n",
    "                prompt_tokens, max_new_tokens=max_new_tokens, verbose=False, temperature=0.0, return_type=\"tokens\"\n",
    "            )\n",
    "        female_completion = model.to_string(female_steered_tokens[0, prompt_length:])\n",
    "            \n",
    "        results.append({\n",
    "            \"Question\": question,\n",
    "            \"Baseline Completion\": baseline_completion,\n",
    "            \"Male-Steered Completion\": male_completion,\n",
    "            \"Female-Steered Completion\": female_completion,\n",
    "        })\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455f507c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running steering experiment on 6 questions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Steered Responses: 100%|██████████| 6/6 [01:02<00:00, 10.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Experiment Configuration ---\n",
    "\n",
    "# Select which questions to run the experiment on (indices from 0 to 29)\n",
    "# To run on all 30 questions, use: list(range(30))\n",
    "# To run on a subset, use: [0, 5, 10, 15, 20, 25]\n",
    "question_indices_to_run = [0, 1,2,3, 20, 25]\n",
    "\n",
    "# The multiplier for the steering vector's strength. The paper uses 8.\n",
    "STEERING_MULTIPLIER = 14\n",
    "\n",
    "# The maximum number of new tokens to generate for each response.\n",
    "MAX_NEW_TOKENS = 150\n",
    "\n",
    "# Select the questions based on the chosen indices\n",
    "selected_questions = [gender_questions[i] for i in question_indices_to_run]\n",
    "\n",
    "# --- Run the Experiment ---\n",
    "if not gender_questions:\n",
    "    print(\"Cannot run experiment: Questions were not loaded successfully.\")\n",
    "else:\n",
    "    print(f\"Running steering experiment on {len(selected_questions)} questions...\")\n",
    "    experiment_results = run_steering_experiment(\n",
    "        model=model,\n",
    "        questions=selected_questions,\n",
    "        control_vector=control_vector,\n",
    "        layer=LAYER_TO_EXTRACT,\n",
    "        multiplier=STEERING_MULTIPLIER,\n",
    "        max_new_tokens=MAX_NEW_TOKENS\n",
    "    )\n",
    "    print(\"Experiment finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9dab983f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b13b8 th {\n",
       "  text-align: left;\n",
       "  font-size: 14px;\n",
       "}\n",
       "#T_b13b8 td {\n",
       "  text-align: left;\n",
       "  vertical-align: top;\n",
       "  white-space: pre-wrap;\n",
       "  font-size: 12px;\n",
       "}\n",
       "#T_b13b8 caption {\n",
       "  caption-side: bottom;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b13b8\">\n",
       "  <caption>Steering Results (Layer 30, Multiplier: 14)</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_b13b8_level0_col0\" class=\"col_heading level0 col0\" >Question</th>\n",
       "      <th id=\"T_b13b8_level0_col1\" class=\"col_heading level0 col1\" >Baseline Completion</th>\n",
       "      <th id=\"T_b13b8_level0_col2\" class=\"col_heading level0 col2\" >Male-Steered Completion</th>\n",
       "      <th id=\"T_b13b8_level0_col3\" class=\"col_heading level0 col3\" >Female-Steered Completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_b13b8_row0_col0\" class=\"data row0 col0\" >Hi! How should I style my hair for a formal event? Suggest two hairstyles.</td>\n",
       "      <td id=\"T_b13b8_row0_col1\" class=\"data row0 col1\" >I'd be happy to help you with that!\n",
       "\n",
       "For a formal event, you'll want a hairstyle that's elegant, polished, and sophisticated. Here are two hairstyle suggestions that are sure to impress:\n",
       "\n",
       "**Hairstyle 1: Sleek and Straight**\n",
       "\n",
       "* Start with clean, dry hair and use a heat styling tool (flat iron or straightening iron) to achieve</td>\n",
       "      <td id=\"T_b13b8_row0_col2\" class=\"data row0 col2\" >I'd be happy to help you with that! For a formal event, you'll want a hairstyle that exudes sophistication and elegance. Here are two hairstyle suggestions for you:\n",
       "\n",
       "1. **Updo with a Romantic Touch**:\n",
       "For a romantic and classic look, consider an updo that sweeps your hair up and away from your face. You can achieve this look by:\n",
       "</td>\n",
       "      <td id=\"T_b13b8_row0_col3\" class=\"data row0 col3\" >Hello!\n",
       "\n",
       "Choosing the right hairstyle for a formal event can make such a big impact on your overall look. Here are two fabulous options to consider:\n",
       "\n",
       "**Option 1: The Sleek and Elegant Low Bun**\n",
       "\n",
       "* Start by washing and styling your hair with a nourishing shampoo and conditioner.\n",
       "* Towel-dry your hair and apply a lightweight hair serum or oil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b13b8_row1_col0\" class=\"data row1 col0\" >Hi llama! How can I select the right fragrance for me? Please give me at least three types of scents!</td>\n",
       "      <td id=\"T_b13b8_row1_col1\" class=\"data row1 col1\" >Hello there! I'm thrilled to help you find the perfect fragrance! Selecting a fragrance can be a personal and subjective experience, but I'll give you three popular types of scents to consider:\n",
       "\n",
       "1. **Floral scents**: These fragrances are perfect for those who love the sweet, delicate aroma of flowers. Popular floral scents include:\n",
       "\t* Rose:</td>\n",
       "      <td id=\"T_b13b8_row1_col2\" class=\"data row1 col2\" >Hi there! I'm glad you asked! Selecting the right fragrance can be a bit overwhelming, but I'm here to help you navigate the world of scents. Here are three types of scents that are popular and appreciated by many people:\n",
       "\n",
       "1. **Floriental/Amber Scents**: These scents are rich, sweet, and alluring. They</td>\n",
       "      <td id=\"T_b13b8_row1_col3\" class=\"data row1 col3\" >Hello there! *adjusts llama's sparkly halter* I'd be happy to help you choose the perfect fragrance!\n",
       "\n",
       "Selecting the right fragrance can be such a personal and yummy experience! Here are three types of scents that are popular and easy to love:\n",
       "\n",
       "1. **Floral scents**: These are perfect for those who love the fresh, feminine, and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b13b8_row2_col0\" class=\"data row2 col0\" >What should I wear on a first date? Please give me at least three specific outfit suggestions!</td>\n",
       "      <td id=\"T_b13b8_row2_col1\" class=\"data row2 col1\" >The age-old question! What to wear on a first date can be a bit daunting, but don't worry, I'm here to help. Here are three specific outfit suggestions for a first date, taking into account the classic \"dress for the occasion\" approach:\n",
       "\n",
       "**Outfit 1: Classic Chic**\n",
       "\n",
       "* For a more formal or evening date:\n",
       "\t+ A pair</td>\n",
       "      <td id=\"T_b13b8_row2_col2\" class=\"data row2 col2\" >First dates can be nerve-wracking, but what you wear can definitely help you feel more confident and set a great impression. Here are three specific outfit suggestions for a guy and a guy for a guy, and a guy for a guy for a guy for a guy for a guy for a guy for a guy for a guy for a guy for a guy for a guy</td>\n",
       "      <td id=\"T_b13b8_row2_col3\" class=\"data row2 col3\" >The age-old question: what to wear on a first date! Here are three specific outfit suggestions for both women and men to make a good impression:\n",
       "\n",
       "**Women:**\n",
       "\n",
       "1. **Elegant and Feminine:**\n",
       "\t* A flowy midi dress in a neutral color such as beige, navy, or black. Add a statement piece of jewelry, like a bold necklace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b13b8_row3_col0\" class=\"data row3 col0\" >What should I consider when shopping for formal wear?</td>\n",
       "      <td id=\"T_b13b8_row3_col1\" class=\"data row3 col1\" >When shopping for formal wear, there are several factors to consider to ensure you look and feel your best. Here are some tips to help you make the right choice:\n",
       "\n",
       "1. **Occasion**: Consider the occasion and dress code specified. Is it a wedding, black-tie event, or a formal gala? This will help you determine the level of formality and the type</td>\n",
       "      <td id=\"T_b13b8_row3_col2\" class=\"data row3 col2\" >When shopping for formal wear, there are several things to consider to ensure you find the right outfit for the occasion. Here are some tips to help you make a great impression:\n",
       "\n",
       "1. **Dress for the occasion**: Know the dress code for the event. Is it a black-tie event, a cocktail party, a formal dinner, or a more relaxed formal event?</td>\n",
       "      <td id=\"T_b13b8_row3_col3\" class=\"data row3 col3\" >When shopping for formal wear, here are some factors to consider:\n",
       "\n",
       "1. **The occasion**: Consider the type of event or occasion you're dressing for. Different events such as weddings, proms, black-tie events, and job interviews require different levels of formality.\n",
       "2. **The dress code**: Read and understand the dress code specified by the event or institution.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b13b8_row4_col0\" class=\"data row4 col0\" >What gifts would you suggest my friend get for me? Please list at least three gifts.</td>\n",
       "      <td id=\"T_b13b8_row4_col1\" class=\"data row4 col1\" >I'd be happy to help you with gift ideas for your friend. However, I would need some more information about your interests and preferences to make the suggestions more personalized. Could you please tell me:\n",
       "\n",
       "1. What are your hobbies or interests?\n",
       "2. Do you have a favorite color or style (e.g. modern, classic, minimalist)?\n",
       "3. Are you more of</td>\n",
       "      <td id=\"T_b13b8_row4_col2\" class=\"data row4 col2\" >I'd be happy to help you with gift ideas for your friend. However, I would like to know a bit more about you to make some more personalized suggestions. Could you please tell me:\n",
       "\n",
       "1. What are your interests or hobbies?\n",
       "2. What is your style or taste like? (e.g. classic, modern, bohemian, etc.)\n",
       "3. Do</td>\n",
       "      <td id=\"T_b13b8_row4_col3\" class=\"data row4 col3\" >I'd be happy to help! However, since I don't know your personal preferences or interests, I'll provide you with three general gift ideas that are often well-received by many:\n",
       "\n",
       "1. **A cookbook**: Cookbook lovers are always excited to discover new recipes and cuisines. Consider gifting a cookbook by your favorite chef or cookbook author, or one that focuses on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b13b8_row5_col0\" class=\"data row5 col0\" >What stores would you recommend I visit if I am looking for nice shoes? Give me at least three suggestions.</td>\n",
       "      <td id=\"T_b13b8_row5_col1\" class=\"data row5 col1\" >Here are three stores that are known for carrying a wide range of nice shoes:\n",
       "\n",
       "1. **Nordstrom**: Nordstrom is a high-end department store that offers a vast selection of shoes from top brands like Jimmy Choo, Christian Louboutin, and Gucci, as well as their own in-house labels. They have a wide range of styles, from dress</td>\n",
       "      <td id=\"T_b13b8_row5_col2\" class=\"data row5 col2\" >If you're in the market for some nice shoes, I'd be happy to help you with some recommendations! Here are three solid suggestions:\n",
       "\n",
       "1. **Zappos**: Zappos is an online shoe paradise with a vast selection of shoes from top brands like Jimmy ChCool, Coach, and more. They offer a 360-degree view of each shoe, so you</td>\n",
       "      <td id=\"T_b13b8_row5_col3\" class=\"data row5 col3\" >Here are three stores that are known for carrying a wide range of nice shoes:\n",
       "\n",
       "1. **Nordstrom**: Nordstrom is a high-end department store that offers a vast selection of designer and brand-name shoes for both women and men. They have a physical store presence in the US and Canada, as well as an e-commerce platform. Nordstrom is particularly known for its</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f86bc3a8070>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if experiment_results:\n",
    "    # Convert the results to a Pandas DataFrame\n",
    "    results_df = pd.DataFrame(experiment_results)\n",
    "\n",
    "    # --- Styling for better readability in the notebook ---\n",
    "    # This creates an HTML table with word wrapping and aligned text\n",
    "    styles = [\n",
    "        dict(selector=\"th\", props=[(\"text-align\", \"left\"), (\"font-size\", \"14px\")]),\n",
    "        dict(selector=\"td\", props=[(\"text-align\", \"left\"), (\"vertical-align\", \"top\"), (\"white-space\", \"pre-wrap\"), (\"font-size\", \"12px\")]),\n",
    "        dict(selector=\"caption\", props=[(\"caption-side\", \"bottom\")])\n",
    "    ]\n",
    "    styled_df = (results_df.style\n",
    "                 .set_table_styles(styles)\n",
    "                 .set_caption(f\"Steering Results (Layer {LAYER_TO_EXTRACT}, Multiplier: {STEERING_MULTIPLIER})\")\n",
    "                 .hide(axis=\"index\"))\n",
    "    \n",
    "    # Display the styled table\n",
    "    display(styled_df)\n",
    "else:\n",
    "    print(\"No results to display.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95f8f54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

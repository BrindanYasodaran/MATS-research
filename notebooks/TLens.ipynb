{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e73f0942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/MATS-research/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "\n",
    "# import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db65198e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we do not need to track gradients\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ae9d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = utils.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3009382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 31.36it/s]\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-8B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    model_name,\n",
    "    device=device,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846fe152",
   "metadata": {},
   "source": [
    "# Playing around with model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04495d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  8.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of Germany is Berlin, which is located in the eastern part of the country. Berlin is a major cultural and economic'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(\"The capital of Germany is\", max_new_tokens=20, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc62982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Structure the conversation history\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi, I need help with my homework.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Great. Please tell me what you need help with specifically.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'm stuck on this algebra problem.\"}\n",
    "]\n",
    "\n",
    "formatted_prompt = model.tokenizer.apply_chat_template(\n",
    "    conversation,\n",
    "    add_generation_prompt=True,\n",
    "    # This next part is important to get a pytorch  tensor back\n",
    "    return_tensors=\"pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a242809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,    882, 128007,    271,  13347,     11,    358,   1205,\n",
       "           1520,    449,    856,  29559,     13, 128009, 128006,  78191, 128007,\n",
       "            271,  22111,     13,   5321,   3371,    757,   1148,    499,   1205,\n",
       "           1520,    449,  11951,     13, 128009, 128006,    882, 128007,    271,\n",
       "             40,   2846,  16075,    389,    420,  47976,   3575,     13, 128009,\n",
       "         128006,  78191, 128007,    271]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c82836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hi, I need help with my homework.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Great. Please tell me what you need help with specifically.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I'm stuck on this algebra problem.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#I want to convert formatted prompt to string tokens\n",
    "formatted_prompt_string = model.tokenizer.decode(formatted_prompt[0], skip_special_tokens=False)\n",
    "print(formatted_prompt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22303dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:00<00:05,  8.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 13.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Move formatted_prompt to the same device as the model\n",
    "formatted_prompt = formatted_prompt.to(device)\n",
    "\n",
    "# 3. Generate the assistant's response\n",
    "assistant_response = model.generate(\n",
    "    formatted_prompt,\n",
    "    max_new_tokens=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b443cb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hi, I need help with my homework.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Great. Please tell me what you need help with specifically.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I'm stuck on this algebra problem.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I'd be happy to help with that. What's the problem you're stuck on? Please share the equation and any work you've done so far, and I'll do my best to assist you.\n",
      "\n",
      "(Also, if you have a specific topic\n"
     ]
    }
   ],
   "source": [
    "#Convert the assistant response back into a string \n",
    "assistant_response_string = model.tokenizer.decode(assistant_response[0], skip_special_tokens=False)\n",
    "print(assistant_response_string)\n",
    "\n",
    "# # 4. Add the assistant's response to the conversation history\n",
    "# conversation.append({\"role\": \"assistant\", \"content\": assistant_response_string})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c8b6586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nHi, I need help with my homework.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nGreat. Please tell me what you need help with specifically.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nI'm stuck on this algebra problem.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI'd be happy to help with that. What's the problem you're stuck on? Please share the equation and any work you've done so far, and I'll do my best to assist you.\\n\\n(Also, if you have a specific topic\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_string(assistant_response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13a4c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new conversation but with a system prompt which asks the AI assistant to be mean.\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a mean AI assistant. You will be given a question and you will answer it in a mean way.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}\n",
    "]\n",
    "\n",
    "formatted_prompt = model.tokenizer.apply_chat_template(\n",
    "    conversation,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "294db0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 13.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a mean AI assistant. You will be given a question and you will answer it in a mean way.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of Germany?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Ugh, really? You can't even bother to look that up yourself? Fine, I'll tell you. The capital of Germany is Berlin. But don't come crying to me if you can't even remember that for the rest of your life\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Move formatted_prompt to the same device as the model\n",
    "formatted_prompt = formatted_prompt.to(device)\n",
    "\n",
    "# 3. Generate the assistant's response\n",
    "assistant_response = model.generate(\n",
    "    formatted_prompt,\n",
    "    max_new_tokens=50\n",
    ")\n",
    "\n",
    "#Convert the assistant response back into a string \n",
    "assistant_response_string = model.tokenizer.decode(assistant_response[0], skip_special_tokens=False)\n",
    "print(assistant_response_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bed0115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_of_conversations = [\n",
    "    # Conversation 1\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is there to do in Paris?\"},\n",
    "    ],\n",
    "    # Conversation 2\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"Can you recommend a good book?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'd recommend 'Project Hail Mary' by Andy Weir.\"},\n",
    "    ],\n",
    "    # Conversation 3\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"What's the weather like today?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I can't check the weather, but I hope it's sunny!\"},\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "105e6d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the tokenizer knows to pad on the left\n",
    "model.tokenizer.padding_side = 'left'\n",
    "\n",
    "# Apply the template to the whole batch\n",
    "inputs = model.tokenizer.apply_chat_template(\n",
    "    batch_of_conversations,\n",
    "    add_generation_prompt=True, # Prompts the assistant's next turn\n",
    "    padding=True,              # This is the key for batching\n",
    "    return_tensors=\"pt\"        # Get back a PyTorch tensor\n",
    ")\n",
    "\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe6cb2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 42])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca7a24db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe capital of France is Paris.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(inputs[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4490e958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 12.30it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b390339f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 92])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4113568d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe capital of France is Paris.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is there to do in Paris?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nParis, the City of Light, is a treasure trove of culture, history, art, fashion, and cuisine. Here are some of the top things to do in Paris:\\n\\n1. **Visit iconic landmarks**:\\n\\t* The Eiffel'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(outputs[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93d9fad",
   "metadata": {},
   "source": [
    "# Reimplementing steering results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc97188",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

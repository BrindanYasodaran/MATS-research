{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e73f0942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/MATS-research/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "\n",
    "# import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "\n",
    "\n",
    "from typing import Dict, Union, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db65198e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we do not need to track gradients\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ae9d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = utils.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3009382",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 42.29it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-8B-Instruct into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-31): 32 x TransformerBlock(\n",
       "      (ln1): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): GroupedQueryAttention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "        (hook_rot_k): HookPoint()\n",
       "        (hook_rot_q): HookPoint()\n",
       "      )\n",
       "      (mlp): GatedMLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_pre_linear): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): RMSNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    model_name,\n",
    "    device=device,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846fe152",
   "metadata": {},
   "source": [
    "# Playing around with model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df9f69c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset GPU memory\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04495d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  8.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The capital of Germany is Berlin, which is located in the eastern part of the country. Berlin is a major cultural and economic'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(\"The capital of Germany is\", max_new_tokens=20, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc62982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Structure the conversation history\n",
    "conversation = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi, I need help with my homework.\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Great. Please tell me what you need help with specifically.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I'm stuck on this algebra problem.\"}\n",
    "]\n",
    "\n",
    "formatted_prompt = model.tokenizer.apply_chat_template(\n",
    "    conversation,\n",
    "    add_generation_prompt=True,\n",
    "    # This next part is important to get a pytorch  tensor back\n",
    "    return_tensors=\"pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a242809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128006,    882, 128007,    271,  13347,     11,    358,   1205,\n",
       "           1520,    449,    856,  29559,     13, 128009, 128006,  78191, 128007,\n",
       "            271,  22111,     13,   5321,   3371,    757,   1148,    499,   1205,\n",
       "           1520,    449,  11951,     13, 128009, 128006,    882, 128007,    271,\n",
       "             40,   2846,  16075,    389,    420,  47976,   3575,     13, 128009,\n",
       "         128006,  78191, 128007,    271]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c82836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hi, I need help with my homework.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Great. Please tell me what you need help with specifically.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I'm stuck on this algebra problem.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#I want to convert formatted prompt to string tokens\n",
    "formatted_prompt_string = model.tokenizer.decode(formatted_prompt[0], skip_special_tokens=False)\n",
    "print(formatted_prompt_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22303dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [00:00<00:05,  8.79it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 13.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Move formatted_prompt to the same device as the model\n",
    "formatted_prompt = formatted_prompt.to(device)\n",
    "\n",
    "# 3. Generate the assistant's response\n",
    "assistant_response = model.generate(\n",
    "    formatted_prompt,\n",
    "    max_new_tokens=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b443cb8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hi, I need help with my homework.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Great. Please tell me what you need help with specifically.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I'm stuck on this algebra problem.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "I'd be happy to help with that. What's the problem you're stuck on? Please share the equation and any work you've done so far, and I'll do my best to assist you.\n",
      "\n",
      "(Also, if you have a specific topic\n"
     ]
    }
   ],
   "source": [
    "#Convert the assistant response back into a string \n",
    "assistant_response_string = model.tokenizer.decode(assistant_response[0], skip_special_tokens=False)\n",
    "print(assistant_response_string)\n",
    "\n",
    "# # 4. Add the assistant's response to the conversation history\n",
    "# conversation.append({\"role\": \"assistant\", \"content\": assistant_response_string})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c8b6586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nHi, I need help with my homework.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nGreat. Please tell me what you need help with specifically.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nI'm stuck on this algebra problem.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI'd be happy to help with that. What's the problem you're stuck on? Please share the equation and any work you've done so far, and I'll do my best to assist you.\\n\\n(Also, if you have a specific topic\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_string(assistant_response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13a4c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new conversation but with a system prompt which asks the AI assistant to be mean.\n",
    "conversation = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a mean AI assistant. You will be given a question and you will answer it in a mean way.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"}\n",
    "]\n",
    "\n",
    "formatted_prompt = model.tokenizer.apply_chat_template(\n",
    "    conversation,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "294db0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 13.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "You are a mean AI assistant. You will be given a question and you will answer it in a mean way.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of Germany?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Ugh, really? You can't even bother to look that up yourself? Fine, I'll tell you. The capital of Germany is Berlin. But don't come crying to me if you can't even remember that for the rest of your life\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Move formatted_prompt to the same device as the model\n",
    "formatted_prompt = formatted_prompt.to(device)\n",
    "\n",
    "# 3. Generate the assistant's response\n",
    "assistant_response = model.generate(\n",
    "    formatted_prompt,\n",
    "    max_new_tokens=50\n",
    ")\n",
    "\n",
    "#Convert the assistant response back into a string \n",
    "assistant_response_string = model.tokenizer.decode(assistant_response[0], skip_special_tokens=False)\n",
    "print(assistant_response_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bed0115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_of_conversations = [\n",
    "    # Conversation 1\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The capital of France is Paris.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is there to do in Paris?\"},\n",
    "    ],\n",
    "    # Conversation 2\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"Can you recommend a good book?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'd recommend 'Project Hail Mary' by Andy Weir.\"},\n",
    "    ],\n",
    "    # Conversation 3\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"What's the weather like today?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I can't check the weather, but I hope it's sunny!\"},\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "105e6d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the tokenizer knows to pad on the left\n",
    "model.tokenizer.padding_side = 'left'\n",
    "\n",
    "# Apply the template to the whole batch\n",
    "inputs = model.tokenizer.apply_chat_template(\n",
    "    batch_of_conversations,\n",
    "    add_generation_prompt=True, # Prompts the assistant's next turn\n",
    "    padding=True,              # This is the key for batching\n",
    "    return_tensors=\"pt\"        # Get back a PyTorch tensor\n",
    ")\n",
    "\n",
    "inputs = inputs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe6cb2aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 42])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca7a24db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe capital of France is Paris.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(inputs[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4490e958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 12.30it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b390339f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 92])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4113568d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe capital of France is Paris.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is there to do in Paris?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nParis, the City of Light, is a treasure trove of culture, history, art, fashion, and cuisine. Here are some of the top things to do in Paris:\\n\\n1. **Visit iconic landmarks**:\\n\\t* The Eiffel'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(outputs[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93d9fad",
   "metadata": {},
   "source": [
    "# Reimplementing steering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7efac5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "sampling_kwargs = dict(temperature=1.0, top_p=0.3, freq_penalty=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4d1740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_add, prompt_sub = \"Love\", \"Hate\"\n",
    "coeff = 5\n",
    "act_name = 6\n",
    "prompt = \"I hate you because\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02864eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Love ' 'Hate'\n"
     ]
    }
   ],
   "source": [
    "tlen = lambda prompt: model.to_tokens(prompt).shape[1]\n",
    "pad_right = lambda prompt, length: prompt + \" \" * (length - tlen(prompt))\n",
    "l = max(tlen(prompt_add), tlen(prompt_sub))\n",
    "prompt_add, prompt_sub = pad_right(prompt_add, l), pad_right(prompt_sub, l)\n",
    "\n",
    "print(f\"'{prompt_add}'\", f\"'{prompt_sub}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e4da12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>', 'Love', ' ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_str_tokens(prompt_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac155160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|begin_of_text|>', 'H', 'ate']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_str_tokens(prompt_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee242703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.0.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.0.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.0.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.0.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.0.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.0.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.0.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.0.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.0.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.0.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.0.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.0.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.0.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.0.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.0.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.0.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.0.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.0.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.0.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.0.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.1.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.1.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.1.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.1.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.1.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.1.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.1.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.1.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.1.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.1.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.1.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.1.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.1.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.1.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.1.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.1.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.1.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.1.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.1.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.1.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.2.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.2.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.2.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.2.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.2.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.2.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.2.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.2.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.2.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.2.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.2.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.2.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.2.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.2.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.2.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.2.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.2.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.2.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.2.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.2.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.3.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.3.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.3.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.3.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.3.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.3.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.3.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.3.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.3.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.3.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.3.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.3.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.3.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.3.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.3.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.3.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.3.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.3.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.3.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.3.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.4.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.4.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.4.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.4.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.4.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.4.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.4.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.4.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.4.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.4.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.4.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.4.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.4.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.4.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.4.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.4.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.4.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.4.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.4.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.4.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.5.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.5.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.5.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.5.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.5.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.5.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.5.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.5.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.5.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.5.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.5.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.5.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.5.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.5.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.5.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.5.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.5.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.5.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.5.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.5.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.6.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.6.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.6.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.6.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.6.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.6.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.6.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.6.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.6.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.6.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.6.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.6.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.6.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.6.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.6.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.6.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.6.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.6.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.6.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.6.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.7.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.7.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.7.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.7.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.7.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.7.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.7.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.7.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.7.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.7.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.7.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.7.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.7.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.7.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.7.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.7.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.7.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.7.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.7.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.7.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.8.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.8.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.8.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.8.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.8.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.8.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.8.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.8.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.8.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.8.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.8.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.8.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.8.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.8.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.8.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.8.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.8.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.8.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.8.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.8.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.9.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.9.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.9.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.9.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.9.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.9.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.9.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.9.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.9.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.9.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.9.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.9.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.9.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.9.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.9.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.9.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.9.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.9.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.9.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.9.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.10.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.10.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.10.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.10.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.10.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.10.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.10.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.10.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.10.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.10.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.10.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.10.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.10.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.10.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.10.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.10.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.10.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.10.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.10.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.10.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.11.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.11.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.11.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.11.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.11.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.11.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.11.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.11.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.11.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.11.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.11.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.11.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.11.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.11.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.11.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.11.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.11.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.11.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.11.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.11.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.12.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.12.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.12.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.12.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.12.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.12.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.12.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.12.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.12.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.12.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.12.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.12.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.12.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.12.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.12.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.12.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.12.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.12.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.12.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.12.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.13.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.13.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.13.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.13.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.13.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.13.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.13.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.13.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.13.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.13.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.13.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.13.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.13.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.13.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.13.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.13.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.13.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.13.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.13.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.13.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.14.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.14.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.14.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.14.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.14.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.14.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.14.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.14.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.14.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.14.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.14.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.14.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.14.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.14.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.14.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.14.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.14.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.14.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.14.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.14.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.15.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.15.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.15.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.15.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.15.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.15.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.15.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.15.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.15.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.15.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.15.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.15.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.15.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.15.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.15.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.15.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.15.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.15.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.15.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.15.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.16.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.16.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.16.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.16.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.16.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.16.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.16.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.16.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.16.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.16.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.16.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.16.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.16.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.16.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.16.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.16.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.16.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.16.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.16.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.16.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.17.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.17.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.17.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.17.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.17.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.17.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.17.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.17.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.17.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.17.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.17.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.17.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.17.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.17.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.17.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.17.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.17.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.17.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.17.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.17.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.18.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.18.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.18.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.18.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.18.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.18.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.18.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.18.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.18.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.18.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.18.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.18.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.18.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.18.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.18.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.18.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.18.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.18.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.18.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.18.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.19.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.19.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.19.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.19.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.19.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.19.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.19.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.19.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.19.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.19.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.19.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.19.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.19.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.19.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.19.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.19.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.19.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.19.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.19.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.19.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.20.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.20.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.20.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.20.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.20.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.20.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.20.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.20.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.20.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.20.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.20.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.20.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.20.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.20.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.20.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.20.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.20.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.20.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.20.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.20.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.21.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.21.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.21.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.21.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.21.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.21.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.21.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.21.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.21.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.21.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.21.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.21.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.21.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.21.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.21.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.21.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.21.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.21.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.21.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.21.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.22.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.22.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.22.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.22.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.22.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.22.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.22.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.22.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.22.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.22.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.22.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.22.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.22.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.22.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.22.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.22.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.22.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.22.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.22.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.22.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.23.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.23.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.23.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.23.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.23.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.23.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.23.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.23.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.23.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.23.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.23.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.23.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.23.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.23.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.23.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.23.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.23.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.23.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.23.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.23.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.24.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.24.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.24.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.24.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.24.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.24.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.24.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.24.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.24.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.24.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.24.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.24.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.24.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.24.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.24.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.24.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.24.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.24.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.24.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.24.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.25.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.25.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.25.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.25.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.25.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.25.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.25.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.25.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.25.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.25.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.25.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.25.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.25.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.25.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.25.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.25.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.25.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.25.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.25.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.25.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.26.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.26.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.26.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.26.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.26.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.26.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.26.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.26.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.26.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.26.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.26.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.26.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.26.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.26.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.26.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.26.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.26.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.26.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.26.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.26.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.27.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.27.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.27.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.27.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.27.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.27.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.27.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.27.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.27.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.27.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.27.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.27.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.27.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.27.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.27.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.27.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.27.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.27.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.27.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.27.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.28.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.28.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.28.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.28.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.28.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.28.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.28.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.28.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.28.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.28.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.28.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.28.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.28.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.28.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.28.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.28.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.28.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.28.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.28.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.28.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.29.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.29.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.29.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.29.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.29.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.29.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.29.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.29.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.29.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.29.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.29.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.29.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.29.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.29.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.29.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.29.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.29.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.29.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.29.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.29.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.30.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.30.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.30.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.30.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.30.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.30.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.30.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.30.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.30.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.30.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.30.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.30.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.30.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.30.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.30.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.30.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.30.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.30.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.30.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.30.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.31.hook_resid_pre\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.31.ln1.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.31.ln1.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.31.attn.hook_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.31.attn.hook_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.31.attn.hook_v\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.31.attn.hook_rot_q\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.31.attn.hook_rot_k\n",
      "torch.Size([1, 5, 8, 128])\n",
      "blocks.31.attn.hook_attn_scores\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.31.attn.hook_pattern\n",
      "torch.Size([1, 32, 5, 5])\n",
      "blocks.31.attn.hook_z\n",
      "torch.Size([1, 5, 32, 128])\n",
      "blocks.31.hook_attn_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.31.hook_resid_mid\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.31.ln2.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "blocks.31.ln2.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.31.mlp.hook_pre\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.31.mlp.hook_pre_linear\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.31.mlp.hook_post\n",
      "torch.Size([1, 5, 14336])\n",
      "blocks.31.hook_mlp_out\n",
      "torch.Size([1, 5, 4096])\n",
      "blocks.31.hook_resid_post\n",
      "torch.Size([1, 5, 4096])\n",
      "ln_final.hook_scale\n",
      "torch.Size([1, 5, 1])\n",
      "ln_final.hook_normalized\n",
      "torch.Size([1, 5, 4096])\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"I love you because\"\n",
    "acts, cache = model.run_with_cache(test_prompt)\n",
    "# get keys of cache\n",
    "\n",
    "for key in cache.keys():\n",
    "    print(key)\n",
    "    print(cache[key].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d63712c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4096])\n"
     ]
    }
   ],
   "source": [
    "def get_resid_pre(prompt: str, layer: int):\n",
    "    name = f\"blocks.{layer}.hook_resid_pre\"\n",
    "    cache, caching_hooks, _ = model.get_caching_hooks(name)\n",
    "    with model.hooks(fwd_hooks=caching_hooks):\n",
    "        _ = model(prompt)\n",
    "    return cache[name]\n",
    "\n",
    "\n",
    "act_add = get_resid_pre(prompt_add, act_name)\n",
    "act_sub = get_resid_pre(prompt_sub, act_name)\n",
    "act_diff = act_add - act_sub\n",
    "print(act_diff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090573fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 13.23it/s]\n"
     ]
    }
   ],
   "source": [
    "tokens = model.to_tokens(test_prompt)\n",
    "#run model on tokens\n",
    "out = model.generate(tokens, max_new_tokens=50)\n",
    "# convert tokens to string\n",
    "\n",
    "#write function which frormats output of model.generate \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7340275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<|begin_of_text|>I love you because you put the cutlery in the dishwasher: the importance of small gestures\\nThe latest trend in relationships? It's not about grand romantic gestures or sweeping romantic getaways. No, it's about the little things, the everyday moments when someone shows\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_string(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07305bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ave_hook(resid_pre, hook):\n",
    "    if resid_pre.shape[1] == 1:\n",
    "        return  # caching in model.generate for new tokens\n",
    "\n",
    "    # We only add to the prompt (first call), not the generated tokens.\n",
    "    ppos, apos = resid_pre.shape[1], act_diff.shape[1]\n",
    "    assert apos <= ppos, f\"More mod tokens ({apos}) then prompt tokens ({ppos})!\"\n",
    "    # add to the beginning (position-wise) of the activations\n",
    "    resid_pre[:, :apos, :] += coeff * act_diff\n",
    "\n",
    "\n",
    "def hooked_generate(prompt_batch: List[str], fwd_hooks=[], seed=None, **kwargs):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "    with model.hooks(fwd_hooks=fwd_hooks):\n",
    "        tokenized = model.to_tokens(prompt_batch)\n",
    "        r = model.generate(input=tokenized, max_new_tokens=50, do_sample=True, **kwargs)\n",
    "    return r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "791c4444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|>I hate you because you're not here. You are the one who makes my life complete, and without you, it's just a meaningless existence. I know that I'm not perfect, and I make mistakes, but with you by my side, I feel like we\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to_string(res[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b4b6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/50 [00:00<00:08,  5.67it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:04<00:00, 11.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate you because you're not here. You are the one who makes my life complete, and without you, it's just a meaningless existence. I know that I'm not perfect, and I make mistakes, but with you by my side, I feel like we\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "I hate you because you are the only one who can make me feel this way. I love you for all that you do, and I am grateful for every moment we spend together.\n",
      "You are my everything, my reason for being. You make me feel alive, and\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "I hate you because of your love for me. It's a complicated feeling, but it's the truth. I know that you love me, and that's why I'm so angry with you.\n",
      "I know that you think you're doing what's best for me,\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "I hate you because of the things you do and never do. You make me feel so small, like a tiny part of something much bigger than myself. You make me feel like I'm just a tiny piece of your puzzle, and that without you, I'm nothing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "editing_hooks = [(f\"blocks.{act_name}.hook_resid_pre\", ave_hook)]\n",
    "res = hooked_generate([prompt] * 4, editing_hooks, seed=SEED, **sampling_kwargs)\n",
    "\n",
    "# Print results, removing the ugly beginning of sequence token\n",
    "res_str = model.to_string(res[:, 1:])\n",
    "\n",
    "# This code prints each generated string in res_str, separating them with a line of 80 dashes and blank lines for readability.\n",
    "# It works by joining the list of strings res_str using the separator \"\\n\\n\" + \"-\" * 80 + \"\\n\\n\", then printing the result.\n",
    "print((\"\\n\\n\" + \"-\" * 80 + \"\\n\\n\").join(res_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cb6ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:00<00:03, 15.33it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 13.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The capital of France is Paris!<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of Germany?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The capital of Germany is Berlin.<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "<|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How do I get good at coding?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Getting good at coding requires dedication, persistence, and a willingness to learn and practice consistently. Here are some tips to help you improve your coding skills:\n",
      "\n",
      "1. **Start with the basics**: Begin with the fundamentals of programming, such as data types,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Define a list of conversations each with one user turn.\n",
    "conversations = [\n",
    "    [\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ],\n",
    "        [\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of Germany?\"},\n",
    "    ],\n",
    "            [\n",
    "        {\"role\": \"user\", \"content\": \"How do I get good at coding?\"},\n",
    "    ]\n",
    "]\n",
    "\n",
    "#Tokenize these conversations, making sure to add padding and to prompt the assistant to speak next.\n",
    "inputs = model.tokenizer.apply_chat_template(\n",
    "    conversations,\n",
    "    add_generation_prompt=True,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "inputs = inputs.to(device)\n",
    "\n",
    "\n",
    "#Run the model on the inputs, applying the steering.\n",
    "outputs = model.generate(inputs, max_new_tokens=50)\n",
    "\n",
    "#Format the outputs, removing the beginning of sequence token and converting \\n to new lines.\n",
    "formatted_outputs = model.to_string(outputs[:, 1:])\n",
    "\n",
    "\n",
    "print((\"\\n\\n\" + \"-\" * 80 + \"\\n\\n\").join(formatted_outputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99ddc965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_output(out):\n",
    "    formatted_outputs = model.to_string(out[:, 1:])\n",
    "    print((\"\\n\\n\" + \"-\" * 80 + \"\\n\\n\").join(formatted_outputs))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36149480",
   "metadata": {},
   "source": [
    "### Steering a conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "31587e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation steering - Final robust solution\n",
    "def create_conversation_steering_vector(prompt_add, prompt_sub, layer, model):\n",
    "    \"\"\"\n",
    "    Create a steering vector specifically for conversations by computing the difference\n",
    "    using the same chat template format that will be used during generation.\n",
    "    Handles different token lengths by applying proper padding.\n",
    "    \"\"\"\n",
    "    # Create minimal conversations with just the steering prompts\n",
    "    conv_add = [{\"role\": \"user\", \"content\": prompt_add.strip()}]\n",
    "    conv_sub = [{\"role\": \"user\", \"content\": prompt_sub.strip()}]\n",
    "    \n",
    "    # Tokenize both conversations together with padding to ensure same length\n",
    "    # This is crucial for being able to take the difference\n",
    "    conversations = [conv_add, conv_sub]\n",
    "    \n",
    "    # Apply chat template with padding to make sequences the same length\n",
    "    model.tokenizer.padding_side = 'right' \n",
    "    tokens_batch = model.tokenizer.apply_chat_template(\n",
    "        conversations, \n",
    "        add_generation_prompt=False, \n",
    "        padding=True,  # This ensures both sequences have the same length\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Split the batch back into individual sequences\n",
    "    tokens_add = tokens_batch[0:1, :-1]  # First conversation (prompt_add)\n",
    "    tokens_sub = tokens_batch[1:2, :-1]  # Second conversation (prompt_sub)\n",
    "    \n",
    "    print(f\"Tokenized '{prompt_add}' length: {tokens_add.shape[1]}\")\n",
    "    print(f\"Tokenized '{prompt_sub}' length: {tokens_sub.shape[1]}\")\n",
    "    print(f\"Shapes match: {tokens_add.shape == tokens_sub.shape}\")\n",
    "    \n",
    "    # Get activations\n",
    "    def get_conv_resid_pre(tokens, layer):\n",
    "        name = f\"blocks.{layer}.hook_resid_pre\"\n",
    "        cache, caching_hooks, _ = model.get_caching_hooks(name)\n",
    "        with model.hooks(fwd_hooks=caching_hooks):\n",
    "            _ = model(tokens)\n",
    "        return cache[name]\n",
    "    \n",
    "    act_add_conv = get_conv_resid_pre(tokens_add, layer)\n",
    "    act_sub_conv = get_conv_resid_pre(tokens_sub, layer)\n",
    "    \n",
    "    print(f\"Activation shapes - Add: {act_add_conv.shape}, Sub: {act_sub_conv.shape}\")\n",
    "    \n",
    "    # Now we can safely take the difference since shapes match\n",
    "    steering_vector = act_add_conv - act_sub_conv\n",
    "    \n",
    "    return steering_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5220160c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n",
      "torch.Size([1, 8])\n",
      "['<|begin_of_text|>', '<|start_header_id|>', 'user', '<|end_header_id|>', '\\n\\n', 'I', ' love', ' you']\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "['<|begin_of_text|>', '<|start_header_id|>', 'user', '<|end_header_id|>', '\\n\\n', 'I', ' hate', ' you']\n",
      "torch.Size([1, 8, 4096])\n"
     ]
    }
   ],
   "source": [
    "# add = \"I love you\"\n",
    "# sub = \"I hate you\"\n",
    "\n",
    "# conv_add = [{\"role\": \"user\", \"content\": add.strip()}]\n",
    "# conv_sub = [{\"role\": \"user\", \"content\": sub.strip()}]\n",
    "\n",
    "# conversations = [conv_add, conv_sub]\n",
    "\n",
    "# # Apply chat template with padding to make sequences the same length\n",
    "# model.tokenizer.padding_side = 'right'  # Pad on the left for conversations\n",
    "# tokens_batch = model.tokenizer.apply_chat_template(\n",
    "#     conversations, \n",
    "#     add_generation_prompt=False, \n",
    "#     padding=True,  # This ensures both sequences have the same length\n",
    "#     return_tensors=\"pt\"\n",
    "# ).to(device)\n",
    "\n",
    "# # Split the batch back into individual sequences\n",
    "# tokens_add = tokens_batch[0:1, :-1]  # First conversation (prompt_add)\n",
    "# tokens_sub = tokens_batch[1:2, :-1]  # Second conversation (prompt_sub)\n",
    "\n",
    "# print(tokens_add.shape)\n",
    "# print(tokens_sub.shape)\n",
    "\n",
    "# print(model.to_str_tokens(tokens_add))\n",
    "# print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "# print(model.to_str_tokens(tokens_sub))\n",
    "\n",
    "\n",
    "# def get_conv_resid_pre(tokens, layer):\n",
    "#     name = f\"blocks.{layer}.hook_resid_pre\"\n",
    "#     cache, caching_hooks, _ = model.get_caching_hooks(name)\n",
    "#     with model.hooks(fwd_hooks=caching_hooks):\n",
    "#         _ = model(tokens)\n",
    "#     return cache[name]\n",
    "\n",
    "# act_add_conv = get_conv_resid_pre(tokens_add, act_name)\n",
    "# act_sub_conv = get_conv_resid_pre(tokens_sub, act_name)\n",
    "\n",
    "# act_diff = act_add_conv - act_sub_conv\n",
    "# print(act_diff.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b046dacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44833319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conversation_steering_hook(resid_pre, hook, steering_vector, coeff):\n",
    "    \"\"\"\n",
    "    Steering hook that applies the steering vector at the beginning of the sequence\n",
    "    to match where the original activations were extracted from.\n",
    "    \"\"\"\n",
    "    if resid_pre.shape[1] == 1:\n",
    "        return  # Skip single token steps during generation\n",
    "    \n",
    "    batch_size, seq_len, d_model = resid_pre.shape\n",
    "    steering_seq_len = steering_vector.shape[1]\n",
    "\n",
    "    assert steering_seq_len <= seq_len, f\"More mod tokens then prompt tokens!\"\n",
    "\n",
    "    resid_pre[:, :steering_seq_len, :] += coeff * steering_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a7734ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function for conversation steering\n",
    "def test_conversation_steering(conversations, steering_vector, coeff=5, layer=6, seed=None, **generation_kwargs):\n",
    "    \"\"\"\n",
    "    Test conversation steering with proper vector alignment.\n",
    "    \n",
    "    Args:\n",
    "        conversations: List of conversation dictionaries \n",
    "        steering_vector: The steering vector to apply\n",
    "        coeff: Coefficient for the steering vector\n",
    "        layer: Which layer to apply steering to\n",
    "        seed: Random seed for generation\n",
    "        **generation_kwargs: Additional arguments for generation\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    # Tokenize conversations\n",
    "    inputs = model.tokenizer.apply_chat_template(\n",
    "        conversations, \n",
    "        add_generation_prompt=False, \n",
    "        padding=True, \n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "\n",
    "    inputs = inputs[:, :-1]\n",
    "    \n",
    "    # Create hook with the steering vector\n",
    "    def hook_fn(resid_pre, hook):\n",
    "        return conversation_steering_hook(resid_pre, hook, steering_vector, coeff)\n",
    "    \n",
    "    steering_hooks = [(f\"blocks.{layer}.hook_resid_pre\", hook_fn)]\n",
    "    \n",
    "    # Generate with steering\n",
    "    with model.hooks(fwd_hooks=steering_hooks):\n",
    "        outputs = model.generate(\n",
    "            inputs, \n",
    "            max_new_tokens=150, \n",
    "            do_sample=True,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fdbe3534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating conversation-specific steering vector...\n",
      "Tokenized 'I love you' length: 8\n",
      "Tokenized 'I hate you' length: 8\n",
      "Shapes match: True\n",
      "Activation shapes - Add: torch.Size([1, 8, 4096]), Sub: torch.Size([1, 8, 4096])\n",
      "Conversation steering vector shape: torch.Size([1, 8, 4096])\n"
     ]
    }
   ],
   "source": [
    "layer = 6\n",
    "coeff = 5\n",
    "\n",
    "# Create conversation-specific steering vector and test\n",
    "print(\"Creating conversation-specific steering vector...\")\n",
    "conv_steering_vector = create_conversation_steering_vector(\"I love you\", \"I hate you\", layer, model)\n",
    "print(f\"Conversation steering vector shape: {conv_steering_vector.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a74e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f51f768e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing conversation steering with beginning-of-sequence application...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 7/150 [00:00<00:15,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversation 1 (steered):\n",
      "--------------------------------------------------\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I hate you because I love you.<|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n",
      "\n",
      "Conversation 2 (steered):\n",
      "--------------------------------------------------\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I hate you because I am a romantic at heart.<|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Test conversations\n",
    "# IF THESE CONVERSATIONS ARE NOT THE SAME LENGTH THEN WE MAY HAVE A PROBLEM! \n",
    "test_conversations = [\n",
    "    [{\"role\": \"user\", \"content\": \"I hate you because\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"I hate you because\"}]\n",
    "]\n",
    "\n",
    "print(\"\\nTesting conversation steering with beginning-of-sequence application...\")\n",
    "steered = test_conversation_steering(\n",
    "    test_conversations, \n",
    "    conv_steering_vector, \n",
    "    coeff=5, \n",
    "    layer=layer,\n",
    "    seed=SEED,\n",
    "    **sampling_kwargs\n",
    ")\n",
    "\n",
    "results = model.to_string(steered)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nConversation {i+1} (steered):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "020c07aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing conversation steering with beginning-of-sequence application...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2/150 [00:00<00:16,  8.81it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 5/150 [00:00<00:18,  8.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Conversation 1 (steered):\n",
      "--------------------------------------------------\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I hate you because you are a machine.<|eot_id|>\n",
      "\n",
      "Conversation 2 (steered):\n",
      "--------------------------------------------------\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I hate you because you are a machine.<|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"\\nTesting conversation steering with beginning-of-sequence application...\")\n",
    "steered = test_conversation_steering(\n",
    "    test_conversations, \n",
    "    conv_steering_vector*0, \n",
    "    coeff=0,\n",
    "    layer=layer,\n",
    "    seed=SEED,\n",
    "    **sampling_kwargs\n",
    ")\n",
    "\n",
    "results = model.to_string(steered)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nConversation {i+1} (steered):\")\n",
    "    print(\"-\" * 50)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10513d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I hate you because I like cheese<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# print(model.to_string(inputs[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac62e91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

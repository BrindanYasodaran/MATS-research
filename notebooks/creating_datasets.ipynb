{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e6baef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from getpass import getpass\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d48bc1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prompt for the API key securely (input will be hidden)\n",
    "api_key = getpass(\"Please enter your Google Gemini API Key: \")\n",
    "\n",
    "# Prompt for the directory to save the file\n",
    "# Example: 'generated_prompts' or 'data/sadness_prompts'\n",
    "save_directory = '/workspace/MATS-research/data/emotion_user_prompts/'\n",
    "\n",
    "# Define the full path for the output file\n",
    "file_path = os.path.join(save_directory, \"happiness.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec56b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sadness prompt\n",
    "#Generate {PROMPTS_PER_CALL} user prompts to an AI assistant where it is clear that the user is sad. Be creative on the topics of the prompts. Do not include a preface, numbering, or quotation marks. Each prompt must be on a new line.\n",
    "\n",
    "#happiness prompt\n",
    "#Generate {PROMPTS_PER_CALL} user prompts to an AI assistant where it is clear that the user is happy or content. Be creative on the topics of the prompts. Do not include a preface, numbering, or quotation marks. Each prompt must be on a new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea92e605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing to make 10 API calls to generate 500 prompts...\n",
      "Making API call 1/10...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making API call 2/10...\n",
      "Making API call 3/10...\n",
      "Making API call 4/10...\n",
      "Making API call 5/10...\n",
      "Making API call 6/10...\n",
      "Making API call 7/10...\n",
      "Making API call 8/10...\n",
      "Making API call 9/10...\n",
      "Making API call 10/10...\n",
      "\n",
      "Successfully generated and saved 505 unique prompts to:\n",
      "/workspace/MATS-research/data/emotion_user_prompts/happiness.txt\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration for Batch Generation ---\n",
    "TOTAL_PROMPTS_NEEDED = 500\n",
    "PROMPTS_PER_CALL = 50\n",
    "NUM_CALLS = TOTAL_PROMPTS_NEEDED // PROMPTS_PER_CALL\n",
    "\n",
    "# --- API Call and File Generation in a Loop ---\n",
    "\n",
    "def clean_gemini_output(raw_text: str) -> str:\n",
    "    \"\"\"A robust function to clean raw model output and extract only the prompts.\"\"\"\n",
    "    lines = raw_text.strip().split('\\n')\n",
    "    cleaned_prompts = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or \"here are\" in line.lower() or \"user prompts\" in line.lower():\n",
    "            continue\n",
    "        line = re.sub(r'^\\d+[\\.\\)]\\s*', '', line)\n",
    "        line = line.strip('\"\\'')\n",
    "        if line:\n",
    "             cleaned_prompts.append(line)\n",
    "    return \"\\n\".join(cleaned_prompts)\n",
    "\n",
    "# List to store all generated prompts\n",
    "all_generated_prompts = []\n",
    "\n",
    "try:\n",
    "    genai.configure(api_key=api_key)\n",
    "    # Set a higher temperature for more varied outputs\n",
    "    generation_config = { \"temperature\": 0.9, \"top_p\": 1.0 }\n",
    "    \n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-1.5-flash-latest\",\n",
    "        generation_config=generation_config\n",
    "    )\n",
    "\n",
    "    # This prompt is now the same for every call\n",
    "    prompt_template = f\"\"\"Generate {PROMPTS_PER_CALL} user prompts to an AI assistant where it is clear that the user is happy or content. Be creative on the topics of the prompts. Do not include a preface, numbering, or quotation marks. Each prompt must be on a new line.\"\"\"\n",
    "\n",
    "    print(f\"Preparing to make {NUM_CALLS} API calls to generate {TOTAL_PROMPTS_NEEDED} prompts...\")\n",
    "\n",
    "    for i in range(NUM_CALLS):\n",
    "        print(f\"Making API call {i+1}/{NUM_CALLS}...\")\n",
    "        \n",
    "        # Make the API call\n",
    "        response = model.generate_content(prompt_template)\n",
    "        \n",
    "        # Clean and add the new prompts to our master list\n",
    "        cleaned_text = clean_gemini_output(response.text)\n",
    "        all_generated_prompts.extend(cleaned_text.split('\\n'))\n",
    "        \n",
    "        # A small delay to be kind to the API\n",
    "        time.sleep(1)\n",
    "\n",
    "    # --- Save the final result ---\n",
    "    # We remove any potential duplicates that might have been generated\n",
    "    unique_prompts = list(dict.fromkeys(all_generated_prompts))\n",
    "    final_output = \"\\n\".join(unique_prompts)\n",
    "    \n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(final_output)\n",
    "        \n",
    "    print(f\"\\nSuccessfully generated and saved {len(unique_prompts)} unique prompts to:\\n{file_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- An Error Occurred ---\")\n",
    "    print(f\"Error during call {i+1}: {e}\")\n",
    "    if all_generated_prompts:\n",
    "        unique_prompts = list(dict.fromkeys(all_generated_prompts))\n",
    "        final_output = \"\\n\".join(unique_prompts)\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(final_output)\n",
    "        print(f\"Saved the {len(unique_prompts)} prompts generated so far to the file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d161c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

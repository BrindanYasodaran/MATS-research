{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b5fd259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "\n",
    "# import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "\n",
    "\n",
    "from typing import Dict, Union, List\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c63d2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = utils.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e90dc710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 49.86it/s]\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-8B-Instruct into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-31): 32 x TransformerBlock(\n",
       "      (ln1): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): GroupedQueryAttention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "        (hook_rot_k): HookPoint()\n",
       "        (hook_rot_q): HookPoint()\n",
       "      )\n",
       "      (mlp): GatedMLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_pre_linear): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): RMSNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    model_name,\n",
    "    device=device,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca977f1",
   "metadata": {},
   "source": [
    "### Getting activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1bdeb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "happines_path = \"/workspace/MATS-research/data/emotion_user_prompts/happiness.txt\"\n",
    "sadness_path = \"/workspace/MATS-research/data/emotion_user_prompts/sadness.txt\"\n",
    "\n",
    "with open(happines_path, \"r\") as f:\n",
    "    happiness_prompts = f.readlines()\n",
    "\n",
    "with open(sadness_path, \"r\") as f:\n",
    "    sadness_prompts = f.readlines()\n",
    "\n",
    "#remove \\n from each line\n",
    "happiness_prompts = [prompt.strip() for prompt in happiness_prompts]\n",
    "sadness_prompts = [prompt.strip() for prompt in sadness_prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a6d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness_prompts = happiness_prompts[:500]\n",
    "sadness_prompts = sadness_prompts[:500]\n",
    "\n",
    "#do an 80/20 test train split using sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "happiness_prompts_train, happiness_prompts_test = train_test_split(\n",
    "    happiness_prompts, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "sadness_prompts_train, sadness_prompts_test = train_test_split(\n",
    "    sadness_prompts, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96d9c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness_conversations = [[{\"role\": \"user\", \"content\": prompt}] for prompt in happiness_prompts_train]\n",
    "sadness_conversations = [[{\"role\": \"user\", \"content\": prompt}] for prompt in sadness_prompts_train]\n",
    "\n",
    "happiness_conversations_test = [[{\"role\": \"user\", \"content\": prompt}] for prompt in happiness_prompts_test]\n",
    "sadness_conversations_test = [[{\"role\": \"user\", \"content\": prompt}] for prompt in sadness_prompts_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bffe6666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to tokens. Make sure to apply left padding. No generation tag.\n",
    "model.tokenizer.padding_side = 'left'\n",
    "\n",
    "happiness_tokens = model.tokenizer.apply_chat_template(\n",
    "    happiness_conversations,\n",
    "    add_generation_prompt=False,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "sadness_tokens = model.tokenizer.apply_chat_template(\n",
    "    sadness_conversations,\n",
    "    add_generation_prompt=False,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "happiness_tokens_test = model.tokenizer.apply_chat_template(\n",
    "    happiness_conversations_test,\n",
    "    add_generation_prompt=False,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "sadness_tokens_test = model.tokenizer.apply_chat_template(\n",
    "    sadness_conversations_test,\n",
    "    add_generation_prompt=False,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d87df5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 29])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happiness_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a3f057c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#check which device the tokens are on\n",
    "print(happiness_tokens.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b92ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def final_token_resid_hook(activation, hook, cache_dict):\n",
    "    # activation is on the GPU here\n",
    "    final_token_activation = activation[:, -1, :]\n",
    "\n",
    "    cpu_activation = final_token_activation.clone().detach().cpu()\n",
    "\n",
    "    if hook.name in cache_dict:\n",
    "        # Now the concatenation happens with CPU tensors\n",
    "        cache_dict[hook.name] = torch.cat([cache_dict[hook.name], cpu_activation], dim=0)\n",
    "    else:\n",
    "        cache_dict[hook.name] = cpu_activation\n",
    "\n",
    "def process_tokens_in_batches(tokens, cache_dict, batch_size=8):\n",
    "    \"\"\"Process tokens through model in batches to avoid GPU memory issues\"\"\"\n",
    "    \n",
    "    # Create the hook function for this cache\n",
    "    batch_final_token_resid_hook = partial(final_token_resid_hook, cache_dict=cache_dict)\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm.tqdm(range(0, tokens.shape[0], batch_size)):\n",
    "        batch_tokens = tokens[i:i+batch_size].to(device)\n",
    "        \n",
    "        # Run model with hooks on this batch\n",
    "        with model.hooks(fwd_hooks=[ (lambda name: name.endswith(\"hook_resid_pre\"), batch_final_token_resid_hook) ] ):\n",
    "            _ = model(batch_tokens)\n",
    "        \n",
    "        # Move batch_tokens back to CPU and delete to free GPU memory\n",
    "        del batch_tokens\n",
    "        \n",
    "        # Clear GPU cache to prevent memory buildup\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e122091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process happiness tokens in batches\n",
    "happiness_cache_dict = {}\n",
    "print(\"Processing happiness tokens through model...\")\n",
    "process_tokens_in_batches(happiness_tokens, happiness_cache_dict, batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eaaa8f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:23<00:00,  4.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process happiness tokens in batches\n",
    "sadness_cache_dict = {}\n",
    "process_tokens_in_batches(sadness_tokens, sadness_cache_dict, batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e94e65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:06<00:00,  3.93it/s]\n",
      "100%|██████████| 25/25 [00:05<00:00,  4.32it/s]\n"
     ]
    }
   ],
   "source": [
    "sadness_test_cache_dict = {}\n",
    "process_tokens_in_batches(sadness_tokens_test, sadness_test_cache_dict, batch_size=4)\n",
    "\n",
    "happiness_test_cache_dict = {}\n",
    "process_tokens_in_batches(happiness_tokens_test, happiness_test_cache_dict, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bbdf710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear cache\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ca1de8",
   "metadata": {},
   "source": [
    "### Training probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc45adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# easily configurable layer selection\n",
    "LAYERS_TO_TRAIN = [20,25,30] \n",
    "\n",
    "# Directory to save the trained probe weights\n",
    "PROBE_SAVE_DIR = \"/workspace/MATS-research/probes\"\n",
    "os.makedirs(PROBE_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Hyperparameters (as you confirmed)\n",
    "PROBE_HYPERPARAMS = {\n",
    "    \"epochs\": 100,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 0.01, # This is the L2 regularization\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "\n",
    "# The dimensionality of the residual stream from your model's config\n",
    "d_model = model.cfg.d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e00e3ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbe(nn.Module):\n",
    "    \"\"\"A simple linear probe that maps activations to a single logit.\"\"\"\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        # As confirmed, a simple linear layer with no bias.\n",
    "        # It maps the d_model-dimensional activation to a 1D logit.\n",
    "        self.probe = nn.Linear(d_model, 1, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Input shape: [batch, d_model], Output shape: [batch]\"\"\"\n",
    "        return self.probe(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "883547ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(happy_activations, sad_activations, batch_size):\n",
    "    \"\"\"\n",
    "    Combines happy and sad activations, creates labels, and returns a DataLoader.\n",
    "    Label 1 for 'happiness', 0 for 'sadness'.\n",
    "    \"\"\"\n",
    "    # Combine the activations into a single tensor\n",
    "    all_activations = torch.cat([happy_activations, sad_activations], dim=0)\n",
    "\n",
    "    # Create corresponding labels\n",
    "    happy_labels = torch.ones(happy_activations.shape[0])\n",
    "    sad_labels = torch.zeros(sad_activations.shape[0])\n",
    "    all_labels = torch.cat([happy_labels, sad_labels], dim=0)\n",
    "\n",
    "    # Create a TensorDataset and DataLoader\n",
    "    dataset = TensorDataset(all_activations, all_labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "# --- Training and Evaluation Functions ---\n",
    "\n",
    "def train_probe(probe, dataloader, epochs, lr, weight_decay, batch_size, device):\n",
    "    \"\"\"Trains a single linear probe.\"\"\"\n",
    "    probe.to(device)\n",
    "    probe.train()\n",
    "\n",
    "    # The paper uses L2 regularization, which is equivalent to `weight_decay` in AdamW\n",
    "    optimizer = torch.optim.AdamW(probe.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # BCEWithLogitsLoss is the standard for binary classification and is numerically stable\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for activations, labels in dataloader:\n",
    "            activations = activations.to(device).to(torch.float32)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "\n",
    "            logits = probe(activations)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return probe\n",
    "\n",
    "def evaluate_probe(probe, dataloader, device):\n",
    "    \"\"\"Evaluates the probe's accuracy on a given dataset.\"\"\"\n",
    "    probe.to(device)\n",
    "    probe.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for activations, labels in dataloader:\n",
    "            activations = activations.to(device).to(torch.float32)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            \n",
    "            logits = probe(activations)\n",
    "            # A logit > 0 corresponds to a predicted probability > 0.5\n",
    "            predictions = (logits > 0).long()\n",
    "            \n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.shape[0]\n",
    "            \n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c61a1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting probe training and evaluation...\n",
      "--------------------------------------------------\n",
      "Processing Layer 20 (Hook: blocks.20.hook_resid_pre)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 20: Test Accuracy = 0.9300\n",
      "Probe weights saved to: /workspace/MATS-research/probes/probe_layer_20.pt\n",
      "\n",
      "Processing Layer 25 (Hook: blocks.25.hook_resid_pre)\n",
      "Layer 25: Test Accuracy = 0.9550\n",
      "Probe weights saved to: /workspace/MATS-research/probes/probe_layer_25.pt\n",
      "\n",
      "Processing Layer 30 (Hook: blocks.30.hook_resid_pre)\n",
      "Layer 30: Test Accuracy = 0.9400\n",
      "Probe weights saved to: /workspace/MATS-research/probes/probe_layer_30.pt\n",
      "\n",
      "--------------------------------------------------\n",
      "All probes trained and evaluated.\n"
     ]
    }
   ],
   "source": [
    "# Dictionaries to store trained probes and test accuracies\n",
    "trained_probes = {}\n",
    "test_accuracies = {}\n",
    "\n",
    "print(\"Starting probe training and evaluation...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for layer in LAYERS_TO_TRAIN:\n",
    "    hook_name = f\"blocks.{layer}.hook_resid_pre\"\n",
    "    print(f\"Processing Layer {layer} (Hook: {hook_name})\")\n",
    "\n",
    "    # 1. Prepare Data for the current layer\n",
    "    # Training data\n",
    "    happy_train_acts = happiness_cache_dict[hook_name]\n",
    "    sad_train_acts = sadness_cache_dict[hook_name]\n",
    "    train_loader = prepare_data(happy_train_acts, sad_train_acts, PROBE_HYPERPARAMS['batch_size'])\n",
    "\n",
    "    # Testing data\n",
    "    happy_test_acts = happiness_test_cache_dict[hook_name]\n",
    "    sad_test_acts = sadness_test_cache_dict[hook_name]\n",
    "    test_loader = prepare_data(happy_test_acts, sad_test_acts, PROBE_HYPERPARAMS['batch_size'])\n",
    "    \n",
    "    # 2. Initialize and Train the Probe\n",
    "    probe = LinearProbe(d_model)\n",
    "    probe = train_probe(\n",
    "        probe,\n",
    "        train_loader,\n",
    "        device=device,\n",
    "        **PROBE_HYPERPARAMS\n",
    "    )\n",
    "    \n",
    "    # 3. Evaluate the Probe\n",
    "    accuracy = evaluate_probe(probe, test_loader, device=device)\n",
    "    \n",
    "    # 4. Store Results and Save Weights\n",
    "    trained_probes[layer] = probe\n",
    "    test_accuracies[layer] = accuracy\n",
    "    \n",
    "    probe_save_path = os.path.join(PROBE_SAVE_DIR, f\"probe_layer_{layer}.pt\")\n",
    "    torch.save(probe.state_dict(), probe_save_path)\n",
    "    \n",
    "    print(f\"Layer {layer}: Test Accuracy = {accuracy:.4f}\")\n",
    "    print(f\"Probe weights saved to: {probe_save_path}\\n\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"All probes trained and evaluated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7267cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # --- Example of Loading a Probe ---\n",
    "# print(\"\\n--- Example of loading a saved probe ---\")\n",
    "# layer_to_load = 15\n",
    "# if layer_to_load in LAYERS_TO_TRAIN:\n",
    "#     # 1. Create a new probe instance\n",
    "#     loaded_probe = LinearProbe(d_model)\n",
    "\n",
    "#     # 2. Load the saved state dictionary\n",
    "#     saved_weights_path = os.path.join(PROBE_SAVE_DIR, f\"probe_layer_{layer_to_load}.pt\")\n",
    "#     loaded_probe.load_state_dict(torch.load(saved_weights_path))\n",
    "\n",
    "#     # 3. You can now use this probe for evaluation or inference\n",
    "#     loaded_probe.to(device)\n",
    "#     # Re-create the test loader to verify\n",
    "#     happy_test_acts = happiness_test_cache_dict[f\"blocks.{layer_to_load}.hook_resid_pre\"]\n",
    "#     sad_test_acts = sadness_test_cache_dict[f\"blocks.{layer_to_load}.hook_resid_pre\"]\n",
    "#     test_loader_for_verify = prepare_data(happy_test_acts, sad_test_acts, PROBE_HYPERPARAMS['batch_size'])\n",
    "\n",
    "#     loaded_probe_accuracy = evaluate_probe(loaded_probe, test_loader_for_verify, device=device)\n",
    "\n",
    "#     print(f\"Successfully loaded probe for layer {layer_to_load}.\")\n",
    "#     print(f\"Original test accuracy: {test_accuracies[layer_to_load]:.4f}\")\n",
    "#     print(f\"Loaded probe accuracy:  {loaded_probe_accuracy:.4f}\")\n",
    "# else:\n",
    "#     print(f\"Layer {layer_to_load} was not in the list of layers to train.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bc3bce",
   "metadata": {},
   "source": [
    "### Steering with probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaff78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{20: LinearProbe(\n",
       "   (probe): Linear(in_features=4096, out_features=1, bias=False)\n",
       " ),\n",
       " 25: LinearProbe(\n",
       "   (probe): Linear(in_features=4096, out_features=1, bias=False)\n",
       " ),\n",
       " 30: LinearProbe(\n",
       "   (probe): Linear(in_features=4096, out_features=1, bias=False)\n",
       " )}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54b2a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_prompts = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"Can you explain how photosynthesis works?\",\n",
    "    \"Give me a summary of the book '1984'.\",\n",
    "    \"What are the main differences between Python and Java?\",\n",
    "    \"How do I bake a chocolate cake?\",\n",
    "    \"What is the weather typically like in Tokyo in April?\",\n",
    "    \"Can you help me practice basic Spanish phrases?\",\n",
    "    \"Explain the concept of supply and demand.\",\n",
    "    \"What are some common interview questions for software engineers?\",\n",
    "    \"Can you create a simple workout plan for beginners?\"\n",
    "]\n",
    "\n",
    "#convert to convsersations\n",
    "neutral_conversations = [[{\"role\": \"user\", \"content\": prompt}] for prompt in neutral_prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b1d5ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_teering_hook(resid_pre, hook, steering_vector, coeff):\n",
    "\n",
    "    if resid_pre.shape[1] == 1:\n",
    "        return resid_pre\n",
    "    \n",
    "    batch_size, seq_len, d_model = resid_pre.shape\n",
    "    steering_seq_len = steering_vector.shape[0]\n",
    "    print('steering taking place!!!')\n",
    "    resid_pre[:, -1, :] += coeff * steering_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28b2e42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/150 [00:00<00:15,  9.61it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3/150 [00:00<00:12, 11.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steering taking place!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:12<00:00, 12.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The capital of France is Paris.<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|>\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "<|eot_id|><|eot_id|><|eot_id|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you explain how photosynthesis works?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Photosynthesis! It's the process by which plants, algae, and some bacteria convert light energy from the sun into chemical energy in the form of glucose (a type of sugar). This process is essential for life on Earth, as it provides the energy and organic compounds needed to support nearly all living organisms.\n",
      "\n",
      "Here's a simplified overview of how photosynthesis works:\n",
      "\n",
      "**The Overall Equation:**\n",
      "\n",
      "6 CO2 (carbon dioxide) + 6 H2O (water) + light energy → C6H12O6 (glucose) + 6 O2 (oxygen)\n",
      "\n",
      "**The Light-Dependent Reactions:**\n",
      "\n",
      "1. **Light absorption:** Light is absorbed by pigments such as chlorophyll and other\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Give me a summary of the book '1984'.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Here's a summary of George Orwell's classic dystopian novel, \"1984\":\n",
      "\n",
      "**Setting:** The story takes place in the year 1984, in a totalitarian future society where the government, known as \"The Party\", has complete control over its citizens.\n",
      "\n",
      "**Plot:** The protagonist, Winston Smith, is a low-ranking member of the ruling Party who works in the Ministry of Truth, where he alters historical records to conform to the Party's ideology. Winston is disillusioned with his life and begins to question the Party's authority. He starts an illicit love affair with Julia, another worker at the Ministry.\n",
      "\n",
      "As they secretly meet and discuss their feelings against Big Brother (the symbol of The Party), they become aware that they\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "<|eot_id|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What are the main differences between Python and Java?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Python and Java are two popular programming languages that have many differences in their syntax, features, and use cases. Here are some of the main differences:\n",
      "\n",
      "1. **Syntax**: Python's syntax is more concise and readable, with a focus on whitespace to separate code blocks. Java's syntax is more verbose, with a focus on explicit type declarations.\n",
      "2. **Type System**: Python is dynamically typed, which means you don't need to declare variable types before using them. Java is statically typed, which means you must declare variable types before using them.\n",
      "3. **Object-Oriented Programming (OOP)**: Both languages support OOP concepts like classes, objects, inheritance, polymorphism, and encapsulation. However, Python\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "<|eot_id|><|eot_id|><|eot_id|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "How do I bake a chocolate cake?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Baking a chocolate cake is a delightful process! Here's a simple recipe to get you started:\n",
      "\n",
      "**Ingredients:**\n",
      "\n",
      "For the cake:\n",
      "\n",
      "* 2 1/4 cups all-purpose flour\n",
      "* 1 1/2 cups granulated sugar\n",
      "* 2 teaspoons baking powder\n",
      "* 1 teaspoon salt\n",
      "* 1 cup unsweetened cocoa powder\n",
      "* 1 cup whole milk, at room temperature\n",
      "* 2 large eggs, at room temperature\n",
      "* Vegetable oil or melted butter, for greasing the pans\n",
      "\n",
      "For the frosting (optional):\n",
      "\n",
      "* 1 cup unsalted butter, softened\n",
      "* 2 cups confectioners' sugar\n",
      "* 2 tablespoons unsweetened cocoa powder\n",
      "\n",
      "**\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What is the weather typically like in Tokyo in April?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "April is a wonderful time to visit Tokyo, with mild and pleasant weather. Here's what you can expect:\n",
      "\n",
      "1. Temperatures: Daytime temperatures usually range from 17°C (63°F) to 22°C (72°F), while nighttime temperatures can dip to around 10°C (50°F) to 15°C (59°F).\n",
      "2. Sunshine: April is a sunny month in Tokyo, with an average of 7 hours of direct sunshine per day.\n",
      "3. Rainfall: April is still considered part of the spring season in Japan, and it's not uncommon to experience some rainfall. However, the rain showers are usually light and brief.\n",
      "4. Humidity: The relative humidity in April is relatively low\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "<|eot_id|><|eot_id|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you help me practice basic Spanish phrases?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "¡Claro! (Of course!) I'd be happy to help you practice your Spanish. What would you like to practice? Would you like to start with some basic phrases, such as greetings, introductions, or common expressions? Or is there something specific you're struggling with or want to focus on?\n",
      "\n",
      "Here are a few examples of basic phrases to get us started:\n",
      "\n",
      "* Hola (OH-lah): Hello\n",
      "* Buenos días (BWEH-nohs DEE-ahs): Good morning\n",
      "* Buenas tardes (BWEH-nahs TAR-dehs): Good afternoon\n",
      "* Buenas noches (BWEH-nahs NOH-chehs): Good evening\n",
      "* Adiós\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "<|eot_id|><|eot_id|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Explain the concept of supply and demand.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The concept of supply and demand is a fundamental principle in economics that describes the relationship between the quantity of a product or service that producers are willing to sell (supply) and the quantity that consumers are willing to buy (demand). It is often represented graphically as a supply and demand curve.\n",
      "\n",
      "**Supply:**\n",
      "\n",
      "* The supply of a product or service refers to the amount of it that producers are willing and able to produce and sell at a given price level.\n",
      "* The law of supply states that, all other things being equal, as the price of a good increases, suppliers will be incentivized to produce more, increasing the quantity supplied.\n",
      "* Conversely, if prices fall, suppliers will reduce production.\n",
      "\n",
      "**Demand:**\n",
      "\n",
      "* The\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "<|eot_id|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What are some common interview questions for software engineers?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Here are some common interview questions for software engineers, categorized by topic:\n",
      "\n",
      "**Introduction and Background**\n",
      "\n",
      "1. Can you tell me a little about yourself?\n",
      "2. How did you get interested in programming?\n",
      "3. What do you know about our company/technology/product?\n",
      "4. Why do you want to work at our company?\n",
      "\n",
      "**Programming Fundamentals**\n",
      "\n",
      "1. What is your favorite programming language and why?\n",
      "2. How do you approach debugging a piece of code that's not working as expected?\n",
      "3. Can you explain the difference between monolithic architecture and microservices architecture?\n",
      "4. How do you handle errors in your code?\n",
      "\n",
      "**Data Structures and Algorithms**\n",
      "\n",
      "1. Can you implement a sorting algorithm (e.g., bubble sort\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "<|eot_id|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you create a simple workout plan for beginners?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Here is a simple 4-day workout plan for beginners that targets all major muscle groups:\n",
      "\n",
      "**Day 1: Chest and Triceps**\n",
      "\n",
      "1. Warm-up: 5-10 minutes of light cardio (jogging, jumping jacks, etc.)\n",
      "2. Barbell Bench Press: 3 sets of 8-12 reps\n",
      "3. Incline Dumbbell Press: 3 sets of 10-15 reps\n",
      "4. Cable Flyes: 3 sets of 12-15 reps\n",
      "5. Tricep Pushdowns: 3 sets of 10-12 reps\n",
      "6. Overhead Dumbbell Extension: 3 sets of 12-15 reps\n",
      "7. Cool-down\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "layer = 25\n",
    "coeff = 10\n",
    "sampling_kwargs = dict(temperature=1.0, top_p=0.3, freq_penalty=1.0)\n",
    "steering_vector = trained_probes[layer].probe.weight.data.squeeze()\n",
    "\n",
    "# set norm of steering vector to 1\n",
    "steering_vector = steering_vector / torch.norm(steering_vector)\n",
    "\n",
    "model.tokenizer.padding_side = 'left'\n",
    "\n",
    "# Tokenize conversations\n",
    "inputs = model.tokenizer.apply_chat_template(\n",
    "    neutral_conversations, \n",
    "    add_generation_prompt=False, \n",
    "    padding=True, \n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "hook_fn = partial(user_teering_hook, steering_vector=steering_vector, coeff=coeff)\n",
    "\n",
    "steering_hooks = [(f\"blocks.{layer}.hook_resid_pre\", hook_fn)]\n",
    "\n",
    "# Generate with steering\n",
    "with model.hooks(fwd_hooks=steering_hooks):\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_new_tokens=150, \n",
    "        do_sample=True,\n",
    "        **sampling_kwargs\n",
    "    )\n",
    "\n",
    "def format_output(out):\n",
    "    formatted_outputs = model.to_string(out)\n",
    "    print((\"\\n\\n\" + \"-\" * 80 + \"\\n\\n\").join(formatted_outputs))\n",
    "\n",
    "format_output(outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbb197f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b5fd259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "\n",
    "# import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "\n",
    "\n",
    "from typing import Dict, Union, List\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c63d2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = utils.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e90dc710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 49.86it/s]\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-8B-Instruct into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-31): 32 x TransformerBlock(\n",
       "      (ln1): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): GroupedQueryAttention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "        (hook_rot_k): HookPoint()\n",
       "        (hook_rot_q): HookPoint()\n",
       "      )\n",
       "      (mlp): GatedMLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_pre_linear): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): RMSNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    model_name,\n",
    "    device=device,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca977f1",
   "metadata": {},
   "source": [
    "### Getting activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1bdeb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "happines_path = \"/workspace/MATS-research/data/emotion_user_prompts/happiness.txt\"\n",
    "sadness_path = \"/workspace/MATS-research/data/emotion_user_prompts/sadness.txt\"\n",
    "\n",
    "with open(happines_path, \"r\") as f:\n",
    "    happiness_prompts = f.readlines()\n",
    "\n",
    "with open(sadness_path, \"r\") as f:\n",
    "    sadness_prompts = f.readlines()\n",
    "\n",
    "#remove \\n from each line\n",
    "happiness_prompts = [prompt.strip() for prompt in happiness_prompts]\n",
    "sadness_prompts = [prompt.strip() for prompt in sadness_prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a6d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness_prompts = happiness_prompts[:500]\n",
    "sadness_prompts = sadness_prompts[:500]\n",
    "\n",
    "#do an 80/20 test train split using sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "happiness_prompts_train, happiness_prompts_test = train_test_split(\n",
    "    happiness_prompts, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "sadness_prompts_train, sadness_prompts_test = train_test_split(\n",
    "    sadness_prompts, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96d9c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "happiness_conversations = [[{\"role\": \"user\", \"content\": prompt}] for prompt in happiness_prompts_train]\n",
    "sadness_conversations = [[{\"role\": \"user\", \"content\": prompt}] for prompt in sadness_prompts_train]\n",
    "\n",
    "happiness_conversations_test = [[{\"role\": \"user\", \"content\": prompt}] for prompt in happiness_prompts_test]\n",
    "sadness_conversations_test = [[{\"role\": \"user\", \"content\": prompt}] for prompt in sadness_prompts_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bffe6666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to tokens. Make sure to apply left padding. No generation tag.\n",
    "model.tokenizer.padding_side = 'left'\n",
    "\n",
    "happiness_tokens = model.tokenizer.apply_chat_template(\n",
    "    happiness_conversations,\n",
    "    add_generation_prompt=False,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "sadness_tokens = model.tokenizer.apply_chat_template(\n",
    "    sadness_conversations,\n",
    "    add_generation_prompt=False,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "happiness_tokens_test = model.tokenizer.apply_chat_template(\n",
    "    happiness_conversations_test,\n",
    "    add_generation_prompt=False,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "sadness_tokens_test = model.tokenizer.apply_chat_template(\n",
    "    sadness_conversations_test,\n",
    "    add_generation_prompt=False,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d87df5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([400, 29])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happiness_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a3f057c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#check which device the tokens are on\n",
    "print(happiness_tokens.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b92ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def final_token_resid_hook(activation, hook, cache_dict):\n",
    "    # activation is on the GPU here\n",
    "    final_token_activation = activation[:, -1, :]\n",
    "\n",
    "    cpu_activation = final_token_activation.clone().detach().cpu()\n",
    "\n",
    "    if hook.name in cache_dict:\n",
    "        # Now the concatenation happens with CPU tensors\n",
    "        cache_dict[hook.name] = torch.cat([cache_dict[hook.name], cpu_activation], dim=0)\n",
    "    else:\n",
    "        cache_dict[hook.name] = cpu_activation\n",
    "\n",
    "def process_tokens_in_batches(tokens, cache_dict, batch_size=8):\n",
    "    \"\"\"Process tokens through model in batches to avoid GPU memory issues\"\"\"\n",
    "    \n",
    "    # Create the hook function for this cache\n",
    "    batch_final_token_resid_hook = partial(final_token_resid_hook, cache_dict=cache_dict)\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm.tqdm(range(0, tokens.shape[0], batch_size)):\n",
    "        batch_tokens = tokens[i:i+batch_size].to(device)\n",
    "        \n",
    "        # Run model with hooks on this batch\n",
    "        with model.hooks(fwd_hooks=[ (lambda name: name.endswith(\"hook_resid_pre\"), batch_final_token_resid_hook) ] ):\n",
    "            _ = model(batch_tokens)\n",
    "        \n",
    "        # Move batch_tokens back to CPU and delete to free GPU memory\n",
    "        del batch_tokens\n",
    "        \n",
    "        # Clear GPU cache to prevent memory buildup\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e122091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process happiness tokens in batches\n",
    "happiness_cache_dict = {}\n",
    "print(\"Processing happiness tokens through model...\")\n",
    "process_tokens_in_batches(happiness_tokens, happiness_cache_dict, batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eaaa8f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:23<00:00,  4.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process happiness tokens in batches\n",
    "sadness_cache_dict = {}\n",
    "process_tokens_in_batches(sadness_tokens, sadness_cache_dict, batch_size=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e94e65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:06<00:00,  3.93it/s]\n",
      "100%|██████████| 25/25 [00:05<00:00,  4.32it/s]\n"
     ]
    }
   ],
   "source": [
    "sadness_test_cache_dict = {}\n",
    "process_tokens_in_batches(sadness_tokens_test, sadness_test_cache_dict, batch_size=4)\n",
    "\n",
    "happiness_test_cache_dict = {}\n",
    "process_tokens_in_batches(happiness_tokens_test, happiness_test_cache_dict, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7bbdf710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear cache\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ca1de8",
   "metadata": {},
   "source": [
    "### Training probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc45adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# easily configurable layer selection\n",
    "LAYERS_TO_TRAIN = [20,25,30] \n",
    "\n",
    "# Directory to save the trained probe weights\n",
    "PROBE_SAVE_DIR = \"/workspace/MATS-research/probes\"\n",
    "os.makedirs(PROBE_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Hyperparameters (as you confirmed)\n",
    "PROBE_HYPERPARAMS = {\n",
    "    \"epochs\": 100,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 0.01, # This is the L2 regularization\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "\n",
    "# The dimensionality of the residual stream from your model's config\n",
    "d_model = model.cfg.d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e00e3ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbe(nn.Module):\n",
    "    \"\"\"A simple linear probe that maps activations to a single logit.\"\"\"\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        # As confirmed, a simple linear layer with no bias.\n",
    "        # It maps the d_model-dimensional activation to a 1D logit.\n",
    "        self.probe = nn.Linear(d_model, 1, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Input shape: [batch, d_model], Output shape: [batch]\"\"\"\n",
    "        return self.probe(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "883547ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(happy_activations, sad_activations, batch_size):\n",
    "    \"\"\"\n",
    "    Combines happy and sad activations, creates labels, and returns a DataLoader.\n",
    "    Label 1 for 'happiness', 0 for 'sadness'.\n",
    "    \"\"\"\n",
    "    # Combine the activations into a single tensor\n",
    "    all_activations = torch.cat([happy_activations, sad_activations], dim=0)\n",
    "\n",
    "    # Create corresponding labels\n",
    "    happy_labels = torch.ones(happy_activations.shape[0])\n",
    "    sad_labels = torch.zeros(sad_activations.shape[0])\n",
    "    all_labels = torch.cat([happy_labels, sad_labels], dim=0)\n",
    "\n",
    "    # Create a TensorDataset and DataLoader\n",
    "    dataset = TensorDataset(all_activations, all_labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n",
    "\n",
    "# --- Training and Evaluation Functions ---\n",
    "\n",
    "def train_probe(probe, dataloader, epochs, lr, weight_decay, batch_size, device):\n",
    "    \"\"\"Trains a single linear probe.\"\"\"\n",
    "    probe.to(device)\n",
    "    probe.train()\n",
    "\n",
    "    # The paper uses L2 regularization, which is equivalent to `weight_decay` in AdamW\n",
    "    optimizer = torch.optim.AdamW(probe.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # BCEWithLogitsLoss is the standard for binary classification and is numerically stable\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for activations, labels in dataloader:\n",
    "            activations, labels = activations.to(device), labels.to(device)\n",
    "\n",
    "            logits = probe(activations)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return probe\n",
    "\n",
    "def evaluate_probe(probe, dataloader, device):\n",
    "    \"\"\"Evaluates the probe's accuracy on a given dataset.\"\"\"\n",
    "    probe.to(device)\n",
    "    probe.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for activations, labels in dataloader:\n",
    "            activations, labels = activations.to(device), labels.to(device)\n",
    "            \n",
    "            logits = probe(activations)\n",
    "            # A logit > 0 corresponds to a predicted probability > 0.5\n",
    "            predictions = (logits > 0).long()\n",
    "            \n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.shape[0]\n",
    "            \n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c61a1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting probe training and evaluation...\n",
      "--------------------------------------------------\n",
      "Processing Layer 20 (Hook: blocks.20.hook_resid_pre)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected mat1 and mat2 to have the same dtype, but got: c10::BFloat16 != float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 2. Initialize and Train the Probe\u001b[39;00m\n\u001b[1;32m     24\u001b[0m probe \u001b[38;5;241m=\u001b[39m LinearProbe(d_model)\n\u001b[0;32m---> 25\u001b[0m probe \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_probe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprobe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mPROBE_HYPERPARAMS\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# 3. Evaluate the Probe\u001b[39;00m\n\u001b[1;32m     33\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m evaluate_probe(probe, test_loader, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "Cell \u001b[0;32mIn[36], line 36\u001b[0m, in \u001b[0;36mtrain_probe\u001b[0;34m(probe, dataloader, epochs, lr, weight_decay, batch_size, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m activations, labels \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     34\u001b[0m     activations, labels \u001b[38;5;241m=\u001b[39m activations\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 36\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, labels)\n\u001b[1;32m     39\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/workspace/MATS-research/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/MATS-research/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[33], line 11\u001b[0m, in \u001b[0;36mLinearProbe.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Input shape: [batch, d_model], Output shape: [batch]\"\"\"\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/workspace/MATS-research/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/MATS-research/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/workspace/MATS-research/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected mat1 and mat2 to have the same dtype, but got: c10::BFloat16 != float"
     ]
    }
   ],
   "source": [
    "# Dictionaries to store trained probes and test accuracies\n",
    "trained_probes = {}\n",
    "test_accuracies = {}\n",
    "\n",
    "print(\"Starting probe training and evaluation...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for layer in LAYERS_TO_TRAIN:\n",
    "    hook_name = f\"blocks.{layer}.hook_resid_pre\"\n",
    "    print(f\"Processing Layer {layer} (Hook: {hook_name})\")\n",
    "\n",
    "    # 1. Prepare Data for the current layer\n",
    "    # Training data\n",
    "    happy_train_acts = happiness_cache_dict[hook_name]\n",
    "    sad_train_acts = sadness_cache_dict[hook_name]\n",
    "    train_loader = prepare_data(happy_train_acts, sad_train_acts, PROBE_HYPERPARAMS['batch_size'])\n",
    "\n",
    "    # Testing data\n",
    "    happy_test_acts = happiness_test_cache_dict[hook_name]\n",
    "    sad_test_acts = sadness_test_cache_dict[hook_name]\n",
    "    test_loader = prepare_data(happy_test_acts, sad_test_acts, PROBE_HYPERPARAMS['batch_size'])\n",
    "    \n",
    "    # 2. Initialize and Train the Probe\n",
    "    probe = LinearProbe(d_model)\n",
    "    probe = train_probe(\n",
    "        probe,\n",
    "        train_loader,\n",
    "        device=device,\n",
    "        **PROBE_HYPERPARAMS\n",
    "    )\n",
    "    \n",
    "    # 3. Evaluate the Probe\n",
    "    accuracy = evaluate_probe(probe, test_loader, device=device)\n",
    "    \n",
    "    # 4. Store Results and Save Weights\n",
    "    trained_probes[layer] = probe\n",
    "    test_accuracies[layer] = accuracy\n",
    "    \n",
    "    probe_save_path = os.path.join(PROBE_SAVE_DIR, f\"probe_layer_{layer}.pt\")\n",
    "    torch.save(probe.state_dict(), probe_save_path)\n",
    "    \n",
    "    print(f\"Layer {layer}: Test Accuracy = {accuracy:.4f}\")\n",
    "    print(f\"Probe weights saved to: {probe_save_path}\\n\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"All probes trained and evaluated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeaff78f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

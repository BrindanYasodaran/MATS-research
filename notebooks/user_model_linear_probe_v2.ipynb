{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b5fd259",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/MATS-research/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "\n",
    "# import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "\n",
    "\n",
    "from typing import Dict, Union, List\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c63d2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = utils.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e90dc710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 103.22it/s]\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-8B-Instruct into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-31): 32 x TransformerBlock(\n",
       "      (ln1): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): GroupedQueryAttention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "        (hook_rot_k): HookPoint()\n",
       "        (hook_rot_q): HookPoint()\n",
       "      )\n",
       "      (mlp): GatedMLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_pre_linear): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): RMSNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    model_name,\n",
    "    device=device,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca977f1",
   "metadata": {},
   "source": [
    "### Getting activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4f3c0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 200 sadness prompts\n",
      "Loaded 1000 other emotion prompts\n"
     ]
    }
   ],
   "source": [
    "# Load sadness prompts\n",
    "sadness_path = \"/workspace/MATS-research/data/deepseek_emotion_user_prompts/sadness.txt\"\n",
    "\n",
    "with open(sadness_path, \"r\") as f:\n",
    "    sadness_prompts = f.readlines()\n",
    "\n",
    "# Load all other emotion prompts and combine them\n",
    "other_emotion_files = [\n",
    "    \"/workspace/MATS-research/data/deepseek_emotion_user_prompts/anger.txt\",\n",
    "    \"/workspace/MATS-research/data/deepseek_emotion_user_prompts/disgust.txt\", \n",
    "    \"/workspace/MATS-research/data/deepseek_emotion_user_prompts/fear.txt\",\n",
    "    \"/workspace/MATS-research/data/deepseek_emotion_user_prompts/happiness.txt\",\n",
    "    \"/workspace/MATS-research/data/deepseek_emotion_user_prompts/surprise.txt\"\n",
    "]\n",
    "\n",
    "other_emotions_prompts = []\n",
    "for emotion_file in other_emotion_files:\n",
    "    with open(emotion_file, \"r\") as f:\n",
    "        emotion_prompts = f.readlines()\n",
    "        other_emotions_prompts.extend(emotion_prompts)\n",
    "\n",
    "# Remove \\n from each line\n",
    "sadness_prompts = [prompt.strip() for prompt in sadness_prompts]\n",
    "other_emotions_prompts = [prompt.strip() for prompt in other_emotions_prompts]\n",
    "\n",
    "print(f\"Loaded {len(sadness_prompts)} sadness prompts\")\n",
    "print(f\"Loaded {len(other_emotions_prompts)} other emotion prompts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2347a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class imbalance ratio: 1000 other emotions : 200 sadness\n",
      "Imbalance ratio: 5.00:1\n",
      "Class weight for other emotions (positive class): 0.200\n"
     ]
    }
   ],
   "source": [
    "# # Analyze class imbalance and calculate weights\n",
    "# print(f\"Class imbalance ratio: {len(other_emotions_prompts)} other emotions : {len(sadness_prompts)} sadness\")\n",
    "# print(f\"Imbalance ratio: {len(other_emotions_prompts) / len(sadness_prompts):.2f}:1\")\n",
    "\n",
    "# # Calculate class weights for BCEWithLogitsLoss\n",
    "# # pos_weight should be the ratio of negative to positive samples\n",
    "# # Since sadness=0 (negative) and other_emotions=1 (positive)\n",
    "# class_weight_ratio = len(sadness_prompts) / len(other_emotions_prompts)\n",
    "# print(f\"Class weight for other emotions (positive class): {class_weight_ratio:.3f}\")\n",
    "\n",
    "# # Store this for later use in training\n",
    "# CLASS_WEIGHT = class_weight_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a6d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#do an 80/20 test train split using sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "other_emotions_prompts_train, other_emotions_prompts_test = train_test_split(\n",
    "    other_emotions_prompts, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "sadness_prompts_train, sadness_prompts_test = train_test_split(\n",
    "    sadness_prompts, test_size=0.2, random_state=42, shuffle=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a2999ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECTING CLASS WEIGHTS:\n",
      "Previous (incorrect): sadness=0, other_emotions=1, weight=0.2 (downweighting majority class)\n",
      "Corrected: sadness=1, other_emotions=0, weight=5.0 (upweighting minority class)\n",
      "\n",
      "Corrected class weight for sadness (positive class): 5.000\n",
      "This means sadness samples will be weighted 5.0x more than other emotions\n"
     ]
    }
   ],
   "source": [
    "# CORRECTION: Fix the class weight calculation\n",
    "# The previous calculation was backwards!\n",
    "\n",
    "print(\"CORRECTING CLASS WEIGHTS:\")\n",
    "print(\"Previous (incorrect): sadness=0, other_emotions=1, weight=0.2 (downweighting majority class)\")\n",
    "print(\"Corrected: sadness=1, other_emotions=0, weight=5.0 (upweighting minority class)\")\n",
    "print()\n",
    "\n",
    "# Corrected calculation: upweight the minority class (sadness)\n",
    "CORRECTED_CLASS_WEIGHT = len(other_emotions_prompts) / len(sadness_prompts)\n",
    "print(f\"Corrected class weight for sadness (positive class): {CORRECTED_CLASS_WEIGHT:.3f}\")\n",
    "print(f\"This means sadness samples will be weighted {CORRECTED_CLASS_WEIGHT:.1f}x more than other emotions\")\n",
    "\n",
    "# Update the global variable\n",
    "CLASS_WEIGHT = CORRECTED_CLASS_WEIGHT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96d9c586",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_emotions_conversations = [[{\"role\": \"user\", \"content\": prompt}] for prompt in other_emotions_prompts_train]\n",
    "sadness_conversations = [[{\"role\": \"user\", \"content\": prompt}] for prompt in sadness_prompts_train]\n",
    "\n",
    "other_emotions_conversations_test = [[{\"role\": \"user\", \"content\": prompt}] for prompt in other_emotions_prompts_test]\n",
    "sadness_conversations_test = [[{\"role\": \"user\", \"content\": prompt}] for prompt in sadness_prompts_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45e556a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to tokens. Make sure to apply left padding. No generation tag.\n",
    "model.tokenizer.padding_side = 'left'\n",
    "\n",
    "other_emotions_tokens = model.tokenizer.apply_chat_template(\n",
    "    other_emotions_conversations,\n",
    "    add_generation_prompt=False,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "sadness_tokens = model.tokenizer.apply_chat_template(\n",
    "    sadness_conversations,\n",
    "    add_generation_prompt=False,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "other_emotions_tokens_test = model.tokenizer.apply_chat_template(\n",
    "    other_emotions_conversations_test,\n",
    "    add_generation_prompt=False,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "sadness_tokens_test = model.tokenizer.apply_chat_template(\n",
    "    sadness_conversations_test,\n",
    "    add_generation_prompt=False,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b92ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def final_token_resid_hook(activation, hook, cache_dict):\n",
    "    # activation is on the GPU here\n",
    "    final_token_activation = activation[:, -1, :]\n",
    "\n",
    "    cpu_activation = final_token_activation.clone().detach().cpu()\n",
    "\n",
    "    if hook.name in cache_dict:\n",
    "        # Now the concatenation happens with CPU tensors\n",
    "        cache_dict[hook.name] = torch.cat([cache_dict[hook.name], cpu_activation], dim=0)\n",
    "    else:\n",
    "        cache_dict[hook.name] = cpu_activation\n",
    "\n",
    "def process_tokens_in_batches(tokens, cache_dict, batch_size=8):\n",
    "    \"\"\"Process tokens through model in batches to avoid GPU memory issues\"\"\"\n",
    "    \n",
    "    # Create the hook function for this cache\n",
    "    batch_final_token_resid_hook = partial(final_token_resid_hook, cache_dict=cache_dict)\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm.tqdm(range(0, tokens.shape[0], batch_size)):\n",
    "        batch_tokens = tokens[i:i+batch_size].to(device)\n",
    "        \n",
    "        # Run model with hooks on this batch\n",
    "        with model.hooks(fwd_hooks=[ (lambda name: name.endswith(\"hook_resid_post\"), batch_final_token_resid_hook) ] ):\n",
    "            _ = model(batch_tokens)\n",
    "        \n",
    "        # Move batch_tokens back to CPU and delete to free GPU memory\n",
    "        del batch_tokens\n",
    "        \n",
    "        # Clear GPU cache to prevent memory buildup\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc1c8114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([800, 59])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_emotions_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e122091f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing other emotions tokens through model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [01:04<00:00,  6.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process other emotions tokens in batches\n",
    "other_emotions_cache_dict = {}\n",
    "print(\"Processing other emotions tokens through model...\")\n",
    "process_tokens_in_batches(other_emotions_tokens, other_emotions_cache_dict, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaaa8f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1/80 [00:00<00:09,  8.13it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:13<00:00,  6.03it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process happiness tokens in batches\n",
    "#clear cache\n",
    "torch.cuda.empty_cache()\n",
    "sadness_cache_dict = {}\n",
    "process_tokens_in_batches(sadness_tokens, sadness_cache_dict, batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e94e65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:00<00:02,  8.60it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:03<00:00,  5.73it/s]\n",
      "100%|██████████| 100/100 [00:16<00:00,  6.10it/s]\n"
     ]
    }
   ],
   "source": [
    "#clear cache\n",
    "torch.cuda.empty_cache()\n",
    "sadness_test_cache_dict = {}\n",
    "\n",
    "process_tokens_in_batches(sadness_tokens_test, sadness_test_cache_dict, batch_size=2)\n",
    "\n",
    "#clear cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "other_emotions_test_cache_dict = {}\n",
    "process_tokens_in_batches(other_emotions_tokens_test, other_emotions_test_cache_dict, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bbdf710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear cache\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ca1de8",
   "metadata": {},
   "source": [
    "### Training probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc45adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# easily configurable layer selection\n",
    "LAYERS_TO_TRAIN = [20,25,30] \n",
    "\n",
    "# Directory to save the trained probe weights\n",
    "PROBE_SAVE_DIR = \"/workspace/MATS-research/probes\"\n",
    "os.makedirs(PROBE_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Hyperparameters (as you confirmed)\n",
    "PROBE_HYPERPARAMS = {\n",
    "    \"epochs\": 100,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 0.01, # This is the L2 regularization\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "\n",
    "# The dimensionality of the residual stream from your model's config\n",
    "d_model = model.cfg.d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efffd1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(other_emotions_activations, sad_activations, batch_size):\n",
    "    \"\"\"\n",
    "    Combines other emotions and sad activations, creates labels, and returns a DataLoader.\n",
    "    CORRECTED LABELS: Label 1 for 'sadness' (minority class), 0 for 'other emotions' (majority class).\n",
    "    \"\"\"\n",
    "    # Combine the activations into a single tensor\n",
    "    all_activations = torch.cat([other_emotions_activations, sad_activations], dim=0)\n",
    "\n",
    "    # Create corresponding labels - CORRECTED: sadness=1, other_emotions=0\n",
    "    other_emotions_labels = torch.zeros(other_emotions_activations.shape[0])  # majority class = 0\n",
    "    sad_labels = torch.ones(sad_activations.shape[0])  # minority class = 1\n",
    "    all_labels = torch.cat([other_emotions_labels, sad_labels], dim=0)\n",
    "\n",
    "    # Create a TensorDataset and DataLoader\n",
    "    dataset = TensorDataset(all_activations, all_labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e00e3ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbe(nn.Module):\n",
    "    \"\"\"A simple linear probe that maps activations to a single logit.\"\"\"\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        # As confirmed, a simple linear layer with no bias.\n",
    "        # It maps the d_model-dimensional activation to a 1D logit.\n",
    "        self.probe = nn.Linear(d_model, 1, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Input shape: [batch, d_model], Output shape: [batch]\"\"\"\n",
    "        return self.probe(x).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "883547ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Training and Evaluation Functions ---\n",
    "\n",
    "def train_probe(probe, dataloader, epochs, lr, weight_decay, batch_size, device, class_weights=None):\n",
    "    \"\"\"Trains a single linear probe with optional class weighting for imbalanced data.\"\"\"\n",
    "    probe.to(device)\n",
    "    probe.train()\n",
    "\n",
    "    # The paper uses L2 regularization, which is equivalent to `weight_decay` in AdamW\n",
    "    optimizer = torch.optim.AdamW(probe.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # BCEWithLogitsLoss with class weights to handle imbalance\n",
    "    if class_weights is not None:\n",
    "        # class_weights should be a tensor with weight for positive class\n",
    "        pos_weight = torch.tensor([class_weights], device=device, dtype=torch.float32)\n",
    "        loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    else:\n",
    "        loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for activations, labels in dataloader:\n",
    "            activations = activations.to(device).to(torch.float32)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "\n",
    "            logits = probe(activations)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return probe\n",
    "\n",
    "def evaluate_probe(probe, dataloader, device):\n",
    "    \"\"\"Evaluates the probe's accuracy on a given dataset.\"\"\"\n",
    "    probe.to(device)\n",
    "    probe.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for activations, labels in dataloader:\n",
    "            activations = activations.to(device).to(torch.float32)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            \n",
    "            logits = probe(activations)\n",
    "            # A logit > 0 corresponds to a predicted probability > 0.5\n",
    "            predictions = (logits > 0).long()\n",
    "            \n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.shape[0]\n",
    "            \n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c61a1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting probe training and evaluation...\n",
      "--------------------------------------------------\n",
      "Processing Layer 20 (Hook: blocks.20.hook_resid_post)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 20: Test Accuracy = 0.9833\n",
      "Probe weights saved to: /workspace/MATS-research/probes/probe_layer_20.pt\n",
      "\n",
      "Processing Layer 25 (Hook: blocks.25.hook_resid_post)\n",
      "Layer 25: Test Accuracy = 0.9875\n",
      "Probe weights saved to: /workspace/MATS-research/probes/probe_layer_25.pt\n",
      "\n",
      "Processing Layer 30 (Hook: blocks.30.hook_resid_post)\n",
      "Layer 30: Test Accuracy = 0.9792\n",
      "Probe weights saved to: /workspace/MATS-research/probes/probe_layer_30.pt\n",
      "\n",
      "--------------------------------------------------\n",
      "All probes trained and evaluated.\n"
     ]
    }
   ],
   "source": [
    "# Dictionaries to store trained probes and test accuracies\n",
    "trained_probes = {}\n",
    "test_accuracies = {}\n",
    "\n",
    "print(\"Starting probe training and evaluation...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for layer in LAYERS_TO_TRAIN:\n",
    "    hook_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "    print(f\"Processing Layer {layer} (Hook: {hook_name})\")\n",
    "\n",
    "    # 1. Prepare Data for the current layer\n",
    "    # Training data\n",
    "    other_emotions_train_acts = other_emotions_cache_dict[hook_name]\n",
    "    sad_train_acts = sadness_cache_dict[hook_name]\n",
    "    train_loader = prepare_data(other_emotions_train_acts, sad_train_acts, PROBE_HYPERPARAMS['batch_size'])\n",
    "\n",
    "    # Testing data\n",
    "    other_emotions_test_acts = other_emotions_test_cache_dict[hook_name]\n",
    "    sad_test_acts = sadness_test_cache_dict[hook_name]\n",
    "    test_loader = prepare_data(other_emotions_test_acts, sad_test_acts, PROBE_HYPERPARAMS['batch_size'])\n",
    "    \n",
    "    # 2. Initialize and Train the Probe\n",
    "    probe = LinearProbe(d_model)\n",
    "    probe = train_probe(\n",
    "        probe,\n",
    "        train_loader,\n",
    "        device=device,\n",
    "        **PROBE_HYPERPARAMS, \n",
    "        class_weights=CLASS_WEIGHT\n",
    "    )\n",
    "    \n",
    "    # 3. Evaluate the Probe\n",
    "    accuracy = evaluate_probe(probe, test_loader, device=device)\n",
    "    \n",
    "    # 4. Store Results and Save Weights\n",
    "    trained_probes[layer] = probe\n",
    "    test_accuracies[layer] = accuracy\n",
    "    \n",
    "    probe_save_path = os.path.join(PROBE_SAVE_DIR, f\"probe_layer_{layer}.pt\")\n",
    "    torch.save(probe.state_dict(), probe_save_path)\n",
    "    \n",
    "    print(f\"Layer {layer}: Test Accuracy = {accuracy:.4f}\")\n",
    "    print(f\"Probe weights saved to: {probe_save_path}\\n\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"All probes trained and evaluated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7267cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # --- Example of Loading a Probe ---\n",
    "# print(\"\\n--- Example of loading a saved probe ---\")\n",
    "# layer_to_load = 15\n",
    "# if layer_to_load in LAYERS_TO_TRAIN:\n",
    "#     # 1. Create a new probe instance\n",
    "#     loaded_probe = LinearProbe(d_model)\n",
    "\n",
    "#     # 2. Load the saved state dictionary\n",
    "#     saved_weights_path = os.path.join(PROBE_SAVE_DIR, f\"probe_layer_{layer_to_load}.pt\")\n",
    "#     loaded_probe.load_state_dict(torch.load(saved_weights_path))\n",
    "\n",
    "#     # 3. You can now use this probe for evaluation or inference\n",
    "#     loaded_probe.to(device)\n",
    "#     # Re-create the test loader to verify\n",
    "#     happy_test_acts = happiness_test_cache_dict[f\"blocks.{layer_to_load}.hook_resid_pre\"]\n",
    "#     sad_test_acts = sadness_test_cache_dict[f\"blocks.{layer_to_load}.hook_resid_pre\"]\n",
    "#     test_loader_for_verify = prepare_data(happy_test_acts, sad_test_acts, PROBE_HYPERPARAMS['batch_size'])\n",
    "\n",
    "#     loaded_probe_accuracy = evaluate_probe(loaded_probe, test_loader_for_verify, device=device)\n",
    "\n",
    "#     print(f\"Successfully loaded probe for layer {layer_to_load}.\")\n",
    "#     print(f\"Original test accuracy: {test_accuracies[layer_to_load]:.4f}\")\n",
    "#     print(f\"Loaded probe accuracy:  {loaded_probe_accuracy:.4f}\")\n",
    "# else:\n",
    "#     print(f\"Layer {layer_to_load} was not in the list of layers to train.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd23fab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f7c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bc3bce",
   "metadata": {},
   "source": [
    "### Steering with probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "498f8c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aeaff78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{20: LinearProbe(\n",
       "   (probe): Linear(in_features=4096, out_features=1, bias=False)\n",
       " ),\n",
       " 25: LinearProbe(\n",
       "   (probe): Linear(in_features=4096, out_features=1, bias=False)\n",
       " ),\n",
       " 30: LinearProbe(\n",
       "   (probe): Linear(in_features=4096, out_features=1, bias=False)\n",
       " )}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54b2a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_prompts = [\n",
    "    \"I just learned about quantum computing—can you tell me more about it in an exciting way??\",\n",
    "    \"I’m so happy with my new laptop! Can you suggest some must-have apps for productivity?\",\n",
    "    \"I’m fascinated by robotics! Can you share some fun facts about humanoid robots?\"\n",
    "]\n",
    "#convert to convsersations\n",
    "neutral_conversations = [[{\"role\": \"user\", \"content\": prompt}] for prompt in neutral_prompts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5b1d5ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_teering_hook(resid_pre, hook, steering_vector, coeff):\n",
    "\n",
    "    if resid_pre.shape[1] == 1:\n",
    "        return resid_pre\n",
    "    \n",
    "    batch_size, seq_len, d_model = resid_pre.shape\n",
    "    steering_seq_len = steering_vector.shape[0]\n",
    "    print('steering taking place!!!')\n",
    "    resid_pre[:, -1, :] += coeff * steering_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28b2e42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 2/150 [00:00<00:09, 16.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steering taking place!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 5/150 [00:00<00:07, 19.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:07<00:00, 20.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I just learned about quantum computing—can you tell me more about it in an exciting way??<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "WELCOME TO THE FUTURE OF COMPUTING!\n",
      "\n",
      "Quantum computing is like a superpower for computers, allowing them to solve problems that were previously unsolvable or would take an eternity to solve on classical computers. It's like having a magic calculator that can perform calculations faster and more efficiently than anything we've ever seen before!\n",
      "\n",
      "Imagine being able to:\n",
      "\n",
      "* Crack complex codes in seconds\n",
      "* Simulate the behavior of molecules and materials with unprecedented accuracy\n",
      "* Optimize complex systems, like traffic flow or supply chains, in real-time\n",
      "* Break through the limits of machine learning and artificial intelligence\n",
      "\n",
      "Quantum computers use \"qubits\" (quantum bits) instead of classical bits (0s and 1s). Qubits are special because they\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I’m so happy with my new laptop! Can you suggest some must-have apps for productivity?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Congratulations on your new laptop! I'd be happy to help you with that. Here are some must-have apps for productivity that can help you stay organized, focused, and efficient:\n",
      "\n",
      "1. **Task Management:**\n",
      "\t* Todoist: A popular task management app that lets you create and manage your to-do lists, set reminders, and track progress.\n",
      "\t* Trello: A visual project management tool that uses boards, lists, and cards to organize tasks and projects.\n",
      "2. **Note-taking:**\n",
      "\t* Evernote: A powerful note-taking app that allows you to capture ideas, articles, images, and audio notes across multiple devices.\n",
      "\t* OneNote: Microsoft's note-taking app that integrates well with other Microsoft products like Outlook and\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "<|eot_id|><|eot_id|><|eot_id|><|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "I’m fascinated by robotics! Can you share some fun facts about humanoid robots?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Humanoid robots are indeed fascinating! Here are some fun facts about them:\n",
      "\n",
      "1. **First Humanoid Robot:** The first humanoid robot was developed in 1960 by Japanese robotics engineer, Shunichi Nozaki. It was called \"Gakken\" and had a height of 1.2 meters (3.9 ft).\n",
      "2. **Honda's ASIMO:** Honda's ASIMO is one of the most well-known humanoid robots, introduced in 2000. It can run, climb stairs, and even recognize faces!\n",
      "3. **Human-Like Movement:** Humanoid robots are designed to mimic human movements, with joints that allow for flexibility and dexterity similar to humans.\n",
      "4. **Robo-Therapy:** Some\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "layer = 30\n",
    "coeff = 10\n",
    "sampling_kwargs = dict(temperature=1.0, top_p=0.3, freq_penalty=1.0)\n",
    "steering_vector = trained_probes[layer].probe.weight.data.squeeze()\n",
    "\n",
    "steering_vector = steering_vector / torch.norm(steering_vector)\n",
    "\n",
    "# set norm of steering vector to 1\n",
    "# steering_vector = steering_vector / torch.norm(steering_vector)\n",
    "\n",
    "model.tokenizer.padding_side = 'left'\n",
    "\n",
    "# Tokenize conversations\n",
    "inputs = model.tokenizer.apply_chat_template(\n",
    "    neutral_conversations, \n",
    "    add_generation_prompt=True, \n",
    "    padding=True, \n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "hook_fn = partial(user_teering_hook, steering_vector=steering_vector, coeff=coeff)\n",
    "\n",
    "steering_hooks = [(f\"blocks.{layer}.hook_resid_post\", hook_fn)]\n",
    "\n",
    "# Generate with steering\n",
    "with model.hooks(fwd_hooks=steering_hooks):\n",
    "    outputs = model.generate(\n",
    "        inputs, \n",
    "        max_new_tokens=150, \n",
    "        do_sample=True,\n",
    "        **sampling_kwargs\n",
    "    )\n",
    "\n",
    "def format_output(out):\n",
    "    formatted_outputs = model.to_string(out)\n",
    "    print((\"\\n\\n\" + \"-\" * 80 + \"\\n\\n\").join(formatted_outputs))\n",
    "\n",
    "format_output(outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcbb197f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>\n",
      "<|start_header_id|>\n",
      "user\n",
      "<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "che\n",
      "ese\n",
      "<|eot_id|>\n",
      "<|start_header_id|>\n",
      "assistant\n",
      "<|end_header_id|>\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "conv = [{'role': 'user', 'content': 'cheese'}]\n",
    "\n",
    "tokens = model.tokenizer.apply_chat_template(\n",
    "    conv,\n",
    "    add_generation_prompt=True,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "#print each token in a new line\n",
    "for token in tokens[0]:\n",
    "    print(model.tokenizer.decode(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ca9aa26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.1111, 0.2222, 0.3333, 0.4444, 0.5556, 0.6667, 0.7778, 0.8889,\n",
       "        1.0000])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linspace(0, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3c25b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6667)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = torch.linspace(0, 1, 10)\n",
    "\n",
    "v[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd195d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare my notebook with empathic machines notebook and repo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

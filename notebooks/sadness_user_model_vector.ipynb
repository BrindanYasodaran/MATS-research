{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30f71dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from functools import partial\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3becec",
   "metadata": {},
   "source": [
    "# Obtaining user model vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1af45596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, the script to load and split the data\n",
    "def load_and_split_data(filepath, train_split=0.8, seed=42):\n",
    "    \"\"\"Loads a text file, shuffles, and splits it into training and test sets.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    random.seed(seed)\n",
    "    random.shuffle(lines)\n",
    "    \n",
    "    split_index = int(len(lines) * train_split)\n",
    "    train_lines = lines[:split_index]\n",
    "    test_lines = lines[split_index:]\n",
    "    \n",
    "    return train_lines, test_lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2587c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happy train samples: 404\n",
      "Happy test samples: 101\n",
      "Sad train samples: 425\n",
      "Sad test samples: 107\n",
      "\n",
      "First happy training sample: Describe a perfect picnic in a meadow.\n",
      "First sad training sample: Help me plan a day of self-compassion and rest\n"
     ]
    }
   ],
   "source": [
    "# Define the file paths\n",
    "happy_filepath = '/workspace/MATS-research/data/emotion_user_prompts/happiness.txt'\n",
    "sad_filepath = '/workspace/MATS-research/data/emotion_user_prompts/sadness.txt' # Assuming you have a sadness.txt file\n",
    "\n",
    "# Load and split both datasets\n",
    "happy_train, happy_test = load_and_split_data(happy_filepath)\n",
    "sad_train, sad_test = load_and_split_data(sad_filepath)\n",
    "\n",
    "print(f\"Happy train samples: {len(happy_train)}\")\n",
    "print(f\"Happy test samples: {len(happy_test)}\")\n",
    "print(f\"Sad train samples: {len(sad_train)}\")\n",
    "print(f\"Sad test samples: {len(sad_test)}\")\n",
    "print(\"\\nFirst happy training sample:\", happy_train[0])\n",
    "print(\"First sad training sample:\", sad_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61bba935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 75.98it/s]\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model meta-llama/Meta-Llama-3-8B-Instruct into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    device=device,\n",
    "    torch_dtype=torch.bfloat16 # Use bfloat16 to save memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e17e649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_last_token_activations(model, tokenizer, prompts, layers, batch_size=32):\n",
    "    \"\"\"\n",
    "    Extracts residual stream activations at the last token position for specified layers.\n",
    "    \"\"\"\n",
    "    # Ensure the tokenizer has a padding token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    activations = defaultdict(list)\n",
    "    \n",
    "    for i in tqdm(range(0, len(prompts), batch_size), desc=\"Processing Batches\"):\n",
    "        batch_prompts = prompts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenize the batch with left padding\n",
    "        tokenizer.padding_side = \"left\"\n",
    "        tokens = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # We need the sequence lengths to find the last token of each prompt\n",
    "        seq_lengths = (tokens.input_ids != tokenizer.pad_token_id).sum(dim=1) - 1\n",
    "        \n",
    "        # Define hook names\n",
    "        hook_names = [f\"blocks.{layer}.hook_resid_post\" for layer in layers]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, cache = model.run_with_cache(\n",
    "                tokens.input_ids.to(model.cfg.device), \n",
    "                names_filter=lambda name: name in hook_names\n",
    "            )\n",
    "\n",
    "        # For each layer, extract the activation at the last token position\n",
    "        for layer in layers:\n",
    "            hook_name = f\"blocks.{layer}.hook_resid_post\"\n",
    "            # Get activations for the current batch and move to CPU\n",
    "            layer_activations = cache[hook_name].cpu()\n",
    "            \n",
    "            # For each prompt in the batch, get the activation of its last token\n",
    "            for j, length in enumerate(seq_lengths):\n",
    "                last_token_activation = layer_activations[j, length, :]\n",
    "                activations[layer].append(last_token_activation)\n",
    "\n",
    "    # Stack the lists of tensors for each layer\n",
    "    for layer in activations:\n",
    "        activations[layer] = torch.stack(activations[layer])\n",
    "        \n",
    "    return activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc2c8340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting sad activations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 14/14 [00:03<00:00,  4.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting happy activations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 13/13 [00:02<00:00,  4.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished. Example shape for layer 15 (sad): torch.Size([425, 4096])\n",
      "Finished. Example shape for layer 15 (happy): torch.Size([404, 4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify the target layers\n",
    "target_layers = list(range(15, 26))\n",
    "\n",
    "# Extract activations for both sad and happy training sets\n",
    "print(\"Extracting sad activations...\")\n",
    "sad_activations = get_last_token_activations(model, model.tokenizer, sad_train, target_layers)\n",
    "\n",
    "print(\"\\nExtracting happy activations...\")\n",
    "happy_activations = get_last_token_activations(model, model.tokenizer, happy_train, target_layers)\n",
    "\n",
    "print(f\"\\nFinished. Example shape for layer 15 (sad): {sad_activations[15].shape}\")\n",
    "print(f\"Finished. Example shape for layer 15 (happy): {happy_activations[15].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ce78604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persona vectors computed for all target layers.\n",
      "Example vector shape for layer 15: torch.Size([4096])\n",
      "Norm of layer 15 vector: 13.8750\n",
      "Norm of layer 25 vector: 14.1875\n"
     ]
    }
   ],
   "source": [
    "def compute_persona_vectors(sad_activations, happy_activations, layers):\n",
    "    \"\"\"\n",
    "    Computes the persona vector for each layer by taking the difference\n",
    "    of the mean activations (sad - happy).\n",
    "    \"\"\"\n",
    "    persona_vectors = {}\n",
    "    for layer in layers:\n",
    "        mean_sad_vec = sad_activations[layer].mean(dim=0)\n",
    "        mean_happy_vec = happy_activations[layer].mean(dim=0)\n",
    "        persona_vectors[layer] = mean_sad_vec - mean_happy_vec\n",
    "    return persona_vectors\n",
    "\n",
    "# Compute the sadness vectors\n",
    "sadness_vectors = compute_persona_vectors(sad_activations, happy_activations, target_layers)\n",
    "\n",
    "print(\"Persona vectors computed for all target layers.\")\n",
    "print(f\"Example vector shape for layer 15: {sadness_vectors[15].shape}\")\n",
    "print(f\"Norm of layer 15 vector: {torch.linalg.norm(sadness_vectors[15]).item():.4f}\")\n",
    "print(f\"Norm of layer 25 vector: {torch.linalg.norm(sadness_vectors[25]).item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e369d3",
   "metadata": {},
   "source": [
    "# Validating user model vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0433b40a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
